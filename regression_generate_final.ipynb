{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4532a158",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import tqdm\n",
    "import torchvision\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "import abstract_gradient_training as agt\n",
    "from abstract_gradient_training import AGTConfig\n",
    "from abstract_gradient_training.bounded_models import IntervalBoundedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2643ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_datasets(csv_path='cubic_with_noise.csv', test_size=0.2, balanced=False, drop=0):\n",
    "    \"\"\"\n",
    "    Load cubic dataset with noise from CSV and return train/test datasets as TensorDatasets.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to the dataset CSV file.\n",
    "        test_size (float): Fraction of data to use for testing.\n",
    "        balanced (bool): Whether to balance the dataset based on the sign of the target.\n",
    "\n",
    "    Returns:\n",
    "        train_dataset, test_dataset (TensorDataset, TensorDataset)\n",
    "    \"\"\"\n",
    "\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(csv_path)\n",
    "    x = torch.tensor(df['x'].values, dtype=torch.float32).unsqueeze(1)\n",
    "    y = torch.tensor(df['y'].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    # Optional \"balancing\" — for example, balance data with y > 0 and y <= 0\n",
    "    if balanced:\n",
    "        y_binary = (y > 0).squeeze()\n",
    "        idx_pos = torch.where(y_binary == 1)[0]\n",
    "        idx_neg = torch.where(y_binary == 0)[0]\n",
    "        n_samples = min(len(idx_pos), len(idx_neg))\n",
    "\n",
    "        # Shuffle and sample\n",
    "        idx_pos = idx_pos[torch.randperm(len(idx_pos))[:n_samples]]\n",
    "        idx_neg = idx_neg[torch.randperm(len(idx_neg))[:n_samples]]\n",
    "        idx = torch.cat([idx_pos, idx_neg])\n",
    "\n",
    "        x = x[idx]\n",
    "        y = y[idx]\n",
    "\n",
    "    # Split into train/test\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Assume x_train and y_train are torch tensors\n",
    "    retain_ratio = 1 - (drop / 100)\n",
    "\n",
    "    # Calculate number of samples to keep\n",
    "    num_total = len(x_train)\n",
    "    num_keep = int(num_total * retain_ratio)\n",
    "\n",
    "    # Slice the data to keep only the first `num_keep` samples\n",
    "    x_train = x_train[:num_keep]\n",
    "    y_train = y_train[:num_keep]\n",
    "\n",
    "    # Wrap in TensorDataset\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    test_dataset = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "\n",
    "    return train_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3111b765",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "batchsize = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f0c6a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataloaders\n",
    "dataset_train, dataset_test = get_datasets(balanced=True) \n",
    "torch.manual_seed(0)\n",
    "\n",
    "dl_train = torch.utils.data.DataLoader(dataset_train, batch_size=batchsize, shuffle=True)\n",
    "dl_test = torch.utils.data.DataLoader(dataset_test, batch_size=batchsize, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a73834a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from abstract_gradient_training.bounded_models import BoundedModel\n",
    "def noisy_test_mse(\n",
    "    model: torch.nn.Sequential | BoundedModel,\n",
    "    batch: torch.Tensor,\n",
    "    labels: torch.Tensor,\n",
    "    noise_level: float | torch.Tensor = 0.0,\n",
    "    noise_type: str = \"laplace\",\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Given a pytorch (or bounded) model, calculate the prediction accuracy on a batch of the test set when adding the\n",
    "    specified noise to the predictions.\n",
    "    NOTE: For now, this function only supports binary classification via the noise + threshold dp mechanism. This\n",
    "          should be extended to support multi-class problems via the noisy-argmax mechanism in the future.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Sequential | BoundedModel): The model to evaluate.\n",
    "        batch (torch.Tensor): Input batch of data (shape [batchsize, ...]).\n",
    "        labels (torch.Tensor): Targets for the input batch (shape [batchsize, ]).\n",
    "        noise_level (float | torch.Tensor, optional): Noise level for privacy-preserving predictions using the laplace\n",
    "            mechanism. Can either be a float or a torch.Tensor of shape (batchsize, ).\n",
    "        noise_type (str, optional): Type of noise to add to the predictions, one of [\"laplace\", \"cauchy\"].\n",
    "\n",
    "    Returns:\n",
    "        float: The noisy accuracy of the model on the test set.\n",
    "    \"\"\"\n",
    "    # get the test batch and send it to the correct device\n",
    "    if isinstance(model, BoundedModel):\n",
    "        device = torch.device(model.device) if model.device != -1 else torch.device(\"cpu\")\n",
    "    else:\n",
    "        device = torch.device(next(model.parameters()).device)\n",
    "    batch = batch.to(device)\n",
    "    \n",
    "    # validate the labels\n",
    "    if labels.dim() > 1:\n",
    "        labels = labels.squeeze()\n",
    "        \n",
    "    labels = labels.to(device).type(torch.float64)\n",
    "    assert labels.dim() == 1, \"Labels must be of shape (batchsize, )\"\n",
    "\n",
    "    if noise_type in [\"none\"]:\n",
    "        # nominal, lower and upper bounds for the forward pass\n",
    "        y_n = model.forward(batch).squeeze()\n",
    "        return F.mse_loss(y_n, labels.squeeze()).item()\n",
    "\n",
    "    # validate the noise parameters and set up the distribution\n",
    "    assert noise_type in [\"laplace\", \"cauchy\"], f\"Noise type must be one of ['laplace', 'cauchy'], got {noise_type}\"\n",
    "    noise_level += 1e-7  # can't set distributions scale to zero\n",
    "    noise_level = torch.tensor(noise_level) if isinstance(noise_level, float) else noise_level\n",
    "    noise_level = noise_level.to(device).type(batch.dtype)  # type: ignore\n",
    "    noise_level = noise_level.expand(labels.size())\n",
    "    if noise_type == \"laplace\":\n",
    "        noise_distribution = torch.distributions.Laplace(0, noise_level)\n",
    "    else:\n",
    "        noise_distribution = torch.distributions.Cauchy(0, noise_level)\n",
    "\n",
    "    # nominal, lower and upper bounds for the forward pass\n",
    "    y_n = model.forward(batch).squeeze()\n",
    "\n",
    "    # transform 2-logit models to a single output\n",
    "    if y_n.shape[-1] == 2:\n",
    "        y_n = y_n[:, 1] - y_n[:, 0]\n",
    "    if y_n.dim() > 1:\n",
    "        raise NotImplementedError(\"Noisy accuracy is not supported for multi-class classification.\")\n",
    "\n",
    "    # apply noise + threshold dp mechanisim\n",
    "    noise = noise_distribution.sample().to(y_n.device).squeeze()\n",
    "    assert noise.shape == y_n.shape\n",
    "    y_n = y_n + noise\n",
    "    accuracy = F.mse_loss(y_n, labels.squeeze()).item()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1f06d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the AGT configuration\n",
    "batchsize = 1000000\n",
    "nominal_config = AGTConfig(\n",
    "    fragsize=2000,\n",
    "    learning_rate=1,\n",
    "    n_epochs=10,\n",
    "    device=\"cuda:0\",\n",
    "    l2_reg=0.01,\n",
    "    k_private=1,\n",
    "    loss=\"mse\",\n",
    "    log_level=\"INFO\",\n",
    "    lr_decay=2.0,\n",
    "    clip_gamma=1.0,\n",
    "    lr_min=0.001,\n",
    "    optimizer=\"SGDM\", # we'll use SGD with momentum\n",
    "    optimizer_kwargs={\"momentum\": 0.9, \"nesterov\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2477f93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[AGT] [INFO    ] [12:34:36] =================== Starting Privacy Certified Training ===================\n",
      "[AGT] [INFO    ] [12:34:36] Starting epoch 1\n",
      "[AGT] [INFO    ] [12:34:43] Batch 1. Loss (mse): 133.631 <= 133.631 <= 133.631\n",
      "[AGT] [INFO    ] [12:35:12] Dataloader has only one batch per epoch, effective batchsize may be smaller than expected.\n",
      "[AGT] [INFO    ] [12:35:12] Starting epoch 2\n",
      "[AGT] [INFO    ] [12:35:18] Batch 2. Loss (mse): 60.372 <= 60.376 <= 60.379\n",
      "[AGT] [INFO    ] [12:35:48] Starting epoch 3\n",
      "[AGT] [INFO    ] [12:35:54] Batch 3. Loss (mse): 49.072 <= 49.083 <= 49.094\n",
      "[AGT] [INFO    ] [12:36:25] Starting epoch 4\n",
      "[AGT] [INFO    ] [12:36:32] Batch 4. Loss (mse): 47.310 <= 47.334 <= 47.358\n",
      "[AGT] [INFO    ] [12:37:02] Starting epoch 5\n",
      "[AGT] [INFO    ] [12:37:08] Batch 5. Loss (mse): 47.247 <= 47.296 <= 47.344\n",
      "[AGT] [INFO    ] [12:37:38] Starting epoch 6\n",
      "[AGT] [INFO    ] [12:37:43] Batch 6. Loss (mse): 47.548 <= 47.638 <= 47.727\n",
      "[AGT] [INFO    ] [12:38:13] Starting epoch 7\n",
      "[AGT] [INFO    ] [12:38:19] Batch 7. Loss (mse): 48.112 <= 48.273 <= 48.434\n",
      "[AGT] [INFO    ] [12:38:49] Starting epoch 8\n",
      "[AGT] [INFO    ] [12:38:55] Batch 8. Loss (mse): 48.717 <= 48.982 <= 49.247\n",
      "[AGT] [INFO    ] [12:39:24] Starting epoch 9\n",
      "[AGT] [INFO    ] [12:39:30] Batch 9. Loss (mse): 49.266 <= 49.682 <= 50.101\n",
      "[AGT] [INFO    ] [12:40:01] Starting epoch 10\n",
      "[AGT] [INFO    ] [12:40:07] Batch 10. Loss (mse): 49.680 <= 50.314 <= 50.955\n",
      "[AGT] [INFO    ] [12:40:41] Final Eval. Loss (mse): 49.886 <= 50.832 <= 51.791\n",
      "[AGT] [INFO    ] [12:40:41] =================== Finished Privacy Certified Training ===================\n",
      " 14%|█▍        | 1/7 [06:04<36:28, 364.69s/it][AGT] [INFO    ] [12:40:41] =================== Starting Privacy Certified Training ===================\n",
      "[AGT] [INFO    ] [12:40:41] Starting epoch 1\n",
      "[AGT] [INFO    ] [12:40:47] Batch 1. Loss (mse): 133.631 <= 133.631 <= 133.631\n",
      "[AGT] [INFO    ] [12:41:18] Dataloader has only one batch per epoch, effective batchsize may be smaller than expected.\n",
      "[AGT] [INFO    ] [12:41:18] Starting epoch 2\n",
      "[AGT] [INFO    ] [12:41:24] Batch 2. Loss (mse): 60.372 <= 60.376 <= 60.379\n",
      "[AGT] [INFO    ] [12:41:55] Starting epoch 3\n",
      "[AGT] [INFO    ] [12:42:01] Batch 3. Loss (mse): 49.072 <= 49.083 <= 49.094\n",
      "[AGT] [INFO    ] [12:42:31] Starting epoch 4\n",
      "[AGT] [INFO    ] [12:42:37] Batch 4. Loss (mse): 47.310 <= 47.334 <= 47.358\n",
      "[AGT] [INFO    ] [12:43:07] Starting epoch 5\n",
      "[AGT] [INFO    ] [12:43:13] Batch 5. Loss (mse): 47.247 <= 47.296 <= 47.344\n",
      "[AGT] [INFO    ] [12:43:43] Starting epoch 6\n",
      "[AGT] [INFO    ] [12:43:49] Batch 6. Loss (mse): 47.548 <= 47.638 <= 47.727\n",
      "[AGT] [INFO    ] [12:44:18] Starting epoch 7\n",
      "[AGT] [INFO    ] [12:44:24] Batch 7. Loss (mse): 48.112 <= 48.273 <= 48.434\n",
      "[AGT] [INFO    ] [12:44:56] Starting epoch 8\n",
      "[AGT] [INFO    ] [12:45:02] Batch 8. Loss (mse): 48.717 <= 48.982 <= 49.247\n",
      "[AGT] [INFO    ] [12:45:32] Starting epoch 9\n",
      "[AGT] [INFO    ] [12:45:38] Batch 9. Loss (mse): 49.266 <= 49.682 <= 50.101\n",
      "[AGT] [INFO    ] [12:46:07] Starting epoch 10\n",
      "[AGT] [INFO    ] [12:46:13] Batch 10. Loss (mse): 49.680 <= 50.314 <= 50.955\n",
      "[AGT] [INFO    ] [12:46:45] Final Eval. Loss (mse): 49.886 <= 50.832 <= 51.791\n",
      "[AGT] [INFO    ] [12:46:45] =================== Finished Privacy Certified Training ===================\n",
      " 29%|██▊       | 2/7 [12:09<30:23, 364.68s/it][AGT] [INFO    ] [12:46:45] =================== Starting Privacy Certified Training ===================\n",
      "[AGT] [INFO    ] [12:46:45] Starting epoch 1\n",
      "[AGT] [INFO    ] [12:46:51] Batch 1. Loss (mse): 133.631 <= 133.631 <= 133.631\n",
      "[AGT] [INFO    ] [12:47:21] Dataloader has only one batch per epoch, effective batchsize may be smaller than expected.\n",
      "[AGT] [INFO    ] [12:47:21] Starting epoch 2\n",
      "[AGT] [INFO    ] [12:47:27] Batch 2. Loss (mse): 60.372 <= 60.376 <= 60.379\n",
      "[AGT] [INFO    ] [12:47:56] Starting epoch 3\n",
      "[AGT] [INFO    ] [12:48:02] Batch 3. Loss (mse): 49.072 <= 49.083 <= 49.094\n",
      "[AGT] [INFO    ] [12:48:31] Starting epoch 4\n",
      "[AGT] [INFO    ] [12:48:37] Batch 4. Loss (mse): 47.310 <= 47.334 <= 47.358\n",
      "[AGT] [INFO    ] [12:49:06] Starting epoch 5\n",
      "[AGT] [INFO    ] [12:49:12] Batch 5. Loss (mse): 47.247 <= 47.296 <= 47.344\n",
      "[AGT] [INFO    ] [12:49:42] Starting epoch 6\n",
      "[AGT] [INFO    ] [12:49:48] Batch 6. Loss (mse): 47.548 <= 47.638 <= 47.727\n",
      "[AGT] [INFO    ] [12:50:19] Starting epoch 7\n",
      "[AGT] [INFO    ] [12:50:25] Batch 7. Loss (mse): 48.112 <= 48.273 <= 48.434\n",
      "[AGT] [INFO    ] [12:50:55] Starting epoch 8\n",
      "[AGT] [INFO    ] [12:51:00] Batch 8. Loss (mse): 48.717 <= 48.982 <= 49.247\n",
      "[AGT] [INFO    ] [12:51:29] Starting epoch 9\n",
      "[AGT] [INFO    ] [12:51:35] Batch 9. Loss (mse): 49.266 <= 49.682 <= 50.101\n",
      "[AGT] [INFO    ] [12:52:09] Starting epoch 10\n",
      "[AGT] [INFO    ] [12:52:15] Batch 10. Loss (mse): 49.680 <= 50.314 <= 50.955\n",
      "[AGT] [INFO    ] [12:52:47] Final Eval. Loss (mse): 49.886 <= 50.832 <= 51.791\n",
      "[AGT] [INFO    ] [12:52:47] =================== Finished Privacy Certified Training ===================\n",
      " 43%|████▎     | 3/7 [18:11<24:14, 363.53s/it][AGT] [INFO    ] [12:52:47] =================== Starting Privacy Certified Training ===================\n",
      "[AGT] [INFO    ] [12:52:47] Starting epoch 1\n",
      "[AGT] [INFO    ] [12:52:54] Batch 1. Loss (mse): 133.631 <= 133.631 <= 133.631\n",
      "[AGT] [INFO    ] [12:53:23] Dataloader has only one batch per epoch, effective batchsize may be smaller than expected.\n",
      "[AGT] [INFO    ] [12:53:23] Starting epoch 2\n",
      "[AGT] [INFO    ] [12:53:29] Batch 2. Loss (mse): 60.372 <= 60.376 <= 60.379\n",
      "[AGT] [INFO    ] [12:54:00] Starting epoch 3\n",
      "[AGT] [INFO    ] [12:54:06] Batch 3. Loss (mse): 49.072 <= 49.083 <= 49.094\n",
      "[AGT] [INFO    ] [12:54:36] Starting epoch 4\n",
      "[AGT] [INFO    ] [12:54:42] Batch 4. Loss (mse): 47.310 <= 47.334 <= 47.358\n",
      "[AGT] [INFO    ] [12:55:12] Starting epoch 5\n",
      "[AGT] [INFO    ] [12:55:18] Batch 5. Loss (mse): 47.247 <= 47.296 <= 47.344\n",
      "[AGT] [INFO    ] [12:55:50] Starting epoch 6\n",
      "[AGT] [INFO    ] [12:55:55] Batch 6. Loss (mse): 47.548 <= 47.638 <= 47.727\n",
      "[AGT] [INFO    ] [12:56:26] Starting epoch 7\n",
      "[AGT] [INFO    ] [12:56:31] Batch 7. Loss (mse): 48.112 <= 48.273 <= 48.434\n",
      "[AGT] [INFO    ] [12:57:05] Starting epoch 8\n",
      "[AGT] [INFO    ] [12:57:11] Batch 8. Loss (mse): 48.717 <= 48.982 <= 49.247\n",
      "[AGT] [INFO    ] [12:57:42] Starting epoch 9\n",
      "[AGT] [INFO    ] [12:57:48] Batch 9. Loss (mse): 49.266 <= 49.682 <= 50.101\n",
      "[AGT] [INFO    ] [12:58:17] Starting epoch 10\n",
      "[AGT] [INFO    ] [12:58:22] Batch 10. Loss (mse): 49.680 <= 50.314 <= 50.955\n",
      "[AGT] [INFO    ] [12:58:53] Final Eval. Loss (mse): 49.886 <= 50.832 <= 51.791\n",
      "[AGT] [INFO    ] [12:58:53] =================== Finished Privacy Certified Training ===================\n",
      " 57%|█████▋    | 4/7 [24:17<18:13, 364.49s/it][AGT] [INFO    ] [12:58:53] =================== Starting Privacy Certified Training ===================\n",
      "[AGT] [INFO    ] [12:58:53] Starting epoch 1\n",
      "[AGT] [INFO    ] [12:59:00] Batch 1. Loss (mse): 133.631 <= 133.631 <= 133.631\n",
      "[AGT] [INFO    ] [12:59:29] Dataloader has only one batch per epoch, effective batchsize may be smaller than expected.\n",
      "[AGT] [INFO    ] [12:59:29] Starting epoch 2\n",
      "[AGT] [INFO    ] [12:59:34] Batch 2. Loss (mse): 60.372 <= 60.376 <= 60.379\n",
      "[AGT] [INFO    ] [13:00:03] Starting epoch 3\n",
      "[AGT] [INFO    ] [13:00:09] Batch 3. Loss (mse): 49.072 <= 49.083 <= 49.094\n",
      "[AGT] [INFO    ] [13:00:39] Starting epoch 4\n",
      "[AGT] [INFO    ] [13:00:44] Batch 4. Loss (mse): 47.310 <= 47.334 <= 47.358\n",
      "[AGT] [INFO    ] [13:01:14] Starting epoch 5\n",
      "[AGT] [INFO    ] [13:01:21] Batch 5. Loss (mse): 47.247 <= 47.296 <= 47.344\n",
      "[AGT] [INFO    ] [13:01:51] Starting epoch 6\n",
      "[AGT] [INFO    ] [13:01:57] Batch 6. Loss (mse): 47.548 <= 47.638 <= 47.727\n",
      "[AGT] [INFO    ] [13:02:27] Starting epoch 7\n",
      "[AGT] [INFO    ] [13:02:33] Batch 7. Loss (mse): 48.112 <= 48.273 <= 48.434\n",
      "[AGT] [INFO    ] [13:03:01] Starting epoch 8\n",
      "[AGT] [INFO    ] [13:03:07] Batch 8. Loss (mse): 48.717 <= 48.982 <= 49.247\n",
      "[AGT] [INFO    ] [13:03:36] Starting epoch 9\n",
      "[AGT] [INFO    ] [13:03:42] Batch 9. Loss (mse): 49.266 <= 49.682 <= 50.101\n",
      "[AGT] [INFO    ] [13:04:12] Starting epoch 10\n",
      "[AGT] [INFO    ] [13:04:18] Batch 10. Loss (mse): 49.680 <= 50.314 <= 50.955\n",
      "[AGT] [INFO    ] [13:04:49] Final Eval. Loss (mse): 49.886 <= 50.832 <= 51.791\n",
      "[AGT] [INFO    ] [13:04:49] =================== Finished Privacy Certified Training ===================\n",
      " 71%|███████▏  | 5/7 [30:12<12:02, 361.23s/it][AGT] [INFO    ] [13:04:49] =================== Starting Privacy Certified Training ===================\n",
      "[AGT] [INFO    ] [13:04:49] Starting epoch 1\n",
      "[AGT] [INFO    ] [13:04:55] Batch 1. Loss (mse): 133.631 <= 133.631 <= 133.631\n",
      "[AGT] [INFO    ] [13:05:25] Dataloader has only one batch per epoch, effective batchsize may be smaller than expected.\n",
      "[AGT] [INFO    ] [13:05:25] Starting epoch 2\n",
      "[AGT] [INFO    ] [13:05:30] Batch 2. Loss (mse): 60.372 <= 60.376 <= 60.379\n",
      "[AGT] [INFO    ] [13:06:00] Starting epoch 3\n",
      "[AGT] [INFO    ] [13:06:06] Batch 3. Loss (mse): 49.072 <= 49.083 <= 49.094\n",
      "[AGT] [INFO    ] [13:06:34] Starting epoch 4\n",
      "[AGT] [INFO    ] [13:06:40] Batch 4. Loss (mse): 47.310 <= 47.334 <= 47.358\n",
      "[AGT] [INFO    ] [13:07:09] Starting epoch 5\n",
      "[AGT] [INFO    ] [13:07:15] Batch 5. Loss (mse): 47.247 <= 47.296 <= 47.344\n",
      "[AGT] [INFO    ] [13:07:44] Starting epoch 6\n",
      "[AGT] [INFO    ] [13:07:50] Batch 6. Loss (mse): 47.548 <= 47.638 <= 47.727\n",
      "[AGT] [INFO    ] [13:08:20] Starting epoch 7\n",
      "[AGT] [INFO    ] [13:08:26] Batch 7. Loss (mse): 48.112 <= 48.273 <= 48.434\n",
      "[AGT] [INFO    ] [13:08:55] Starting epoch 8\n",
      "[AGT] [INFO    ] [13:09:01] Batch 8. Loss (mse): 48.717 <= 48.982 <= 49.247\n",
      "[AGT] [INFO    ] [13:09:30] Starting epoch 9\n",
      "[AGT] [INFO    ] [13:09:36] Batch 9. Loss (mse): 49.266 <= 49.682 <= 50.101\n",
      "[AGT] [INFO    ] [13:10:06] Starting epoch 10\n",
      "[AGT] [INFO    ] [13:10:12] Batch 10. Loss (mse): 49.680 <= 50.314 <= 50.955\n",
      "[AGT] [INFO    ] [13:10:43] Final Eval. Loss (mse): 49.886 <= 50.832 <= 51.791\n",
      "[AGT] [INFO    ] [13:10:43] =================== Finished Privacy Certified Training ===================\n",
      " 86%|████████▌ | 6/7 [36:06<05:58, 358.77s/it][AGT] [INFO    ] [13:10:43] =================== Starting Privacy Certified Training ===================\n",
      "[AGT] [INFO    ] [13:10:43] Starting epoch 1\n",
      "[AGT] [INFO    ] [13:10:49] Batch 1. Loss (mse): 133.631 <= 133.631 <= 133.631\n",
      "[AGT] [INFO    ] [13:11:18] Dataloader has only one batch per epoch, effective batchsize may be smaller than expected.\n",
      "[AGT] [INFO    ] [13:11:18] Starting epoch 2\n",
      "[AGT] [INFO    ] [13:11:24] Batch 2. Loss (mse): 60.372 <= 60.376 <= 60.379\n",
      "[AGT] [INFO    ] [13:11:53] Starting epoch 3\n",
      "[AGT] [INFO    ] [13:11:59] Batch 3. Loss (mse): 49.072 <= 49.083 <= 49.094\n",
      "[AGT] [INFO    ] [13:12:28] Starting epoch 4\n",
      "[AGT] [INFO    ] [13:12:34] Batch 4. Loss (mse): 47.310 <= 47.334 <= 47.358\n",
      "[AGT] [INFO    ] [13:13:03] Starting epoch 5\n",
      "[AGT] [INFO    ] [13:13:09] Batch 5. Loss (mse): 47.247 <= 47.296 <= 47.344\n",
      "[AGT] [INFO    ] [13:13:38] Starting epoch 6\n",
      "[AGT] [INFO    ] [13:13:44] Batch 6. Loss (mse): 47.548 <= 47.638 <= 47.727\n",
      "[AGT] [INFO    ] [13:14:13] Starting epoch 7\n",
      "[AGT] [INFO    ] [13:14:19] Batch 7. Loss (mse): 48.112 <= 48.273 <= 48.434\n",
      "[AGT] [INFO    ] [13:14:49] Starting epoch 8\n",
      "[AGT] [INFO    ] [13:14:54] Batch 8. Loss (mse): 48.717 <= 48.982 <= 49.247\n",
      "[AGT] [INFO    ] [13:15:24] Starting epoch 9\n",
      "[AGT] [INFO    ] [13:15:30] Batch 9. Loss (mse): 49.266 <= 49.682 <= 50.101\n",
      "[AGT] [INFO    ] [13:16:00] Starting epoch 10\n",
      "[AGT] [INFO    ] [13:16:06] Batch 10. Loss (mse): 49.680 <= 50.314 <= 50.955\n",
      "[AGT] [INFO    ] [13:16:38] Final Eval. Loss (mse): 49.886 <= 50.832 <= 51.791\n",
      "[AGT] [INFO    ] [13:16:38] =================== Finished Privacy Certified Training ===================\n",
      "100%|██████████| 7/7 [42:01<00:00, 360.24s/it]\n"
     ]
    }
   ],
   "source": [
    "# to use privacy-safe certificates, we need to run AGT for a range of k_private values\n",
    "\n",
    "# we'll just pick a reasonable range of k_private values. adding more values will increase the runtime\n",
    "# but also result in tighter privacy results. even a few values are sufficient to demonstrate tighter privacy\n",
    "\n",
    "k_private_values = [1, 2, 5, 10, 20, 50, 100] \n",
    "privacy_bounded_models = {}\n",
    "config = copy.deepcopy(nominal_config)\n",
    "config.log_level = \"INFO\"\n",
    "path = \"path/to/save\"\n",
    "\n",
    "for k_private in tqdm.tqdm(k_private_values):\n",
    "    # update config\n",
    "    config.k_private = k_private\n",
    "    # form bounded model\n",
    "    torch.manual_seed(1)\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(1, 32),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(32, 1),\n",
    "    )\n",
    "    bounded_model = IntervalBoundedModel(model, trainable=True)\n",
    "    dl_train = torch.utils.data.DataLoader(dataset_train, batch_size=batchsize, shuffle=True)\n",
    "    # run AGT\n",
    "    agt.privacy_certified_training(bounded_model, nominal_config, dl_train, dl_test)\n",
    "    privacy_bounded_models[k_private] = bounded_model\n",
    "\n",
    "    path = os.getcwd()\n",
    "    privacy_bounded_models[k_private].save_params(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
