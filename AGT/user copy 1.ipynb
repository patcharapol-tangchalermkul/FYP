{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dabb954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "import sklearn.model_selection\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import abstract_gradient_training as agt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "013b54a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Initialise the halfmoons training data.\"\"\"\n",
    "seed = 0\n",
    "batchsize = 10000  # number of samples per batch\n",
    "test_size = 5000\n",
    "n_users = 1000\n",
    "n_batches = 1  # number of batches per epoch\n",
    "n_epochs = 10  # number of epochs\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "# load the dataset\n",
    "x, y = sklearn.datasets.make_moons(noise=0.1, n_samples=n_batches*batchsize + test_size, random_state=seed)\n",
    "# to make it easier to train, we'll space the moons out a bit and add some polynomial features\n",
    "x[y==0, 1] += 0.2\n",
    "x = np.hstack((x, x**2, (x[:, 0] * x[:, 1])[:, None], x**3))\n",
    "\n",
    "\n",
    "# Train-test split\n",
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "    x, y, test_size=test_size / (n_batches * batchsize + test_size), random_state=seed\n",
    ")\n",
    "\n",
    "\n",
    "# Assign users randomly to each set after the split\n",
    "user_train = np.random.randint(1, n_users + 1, size=len(x_train))\n",
    "user_test = np.random.randint(1, n_users + 1, size=len(x_test))\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "x_train = torch.from_numpy(x_train).float()\n",
    "x_test = torch.from_numpy(x_test).float()\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_test = torch.from_numpy(y_test)\n",
    "user_train = torch.from_numpy(user_train)\n",
    "user_test = torch.from_numpy(user_test)\n",
    "\n",
    "# Combine inputs and both labels into TensorDatasets\n",
    "dataset_train = torch.utils.data.TensorDataset(x_train, user_train, y_train)\n",
    "dataset_test = torch.utils.data.TensorDataset(x_test, user_test, y_test)\n",
    "\n",
    "print(y)\n",
    "\n",
    "# Create DataLoaders\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batchsize, shuffle=True)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batchsize, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e42f0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[AGT] [INFO    ] [01:15:02] =================== Starting Privacy Certified Training ===================\n",
      "[AGT] [INFO    ] [01:15:02] Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[AGT] [INFO    ] [01:15:02] Batch 1. Loss (accuracy): 0.514 <= 0.514 <= 0.514\n",
      "[AGT] [INFO    ] [01:16:46] Dataloader has only one batch per epoch, effective batchsize may be smaller than expected.\n",
      "[AGT] [INFO    ] [01:16:46] Starting epoch 2\n",
      "[AGT] [INFO    ] [01:16:46] Batch 2. Loss (accuracy): 0.844 <= 0.849 <= 0.854\n",
      "[AGT] [INFO    ] [01:18:29] Starting epoch 3\n",
      "[AGT] [INFO    ] [01:18:29] Batch 3. Loss (accuracy): 0.839 <= 0.847 <= 0.852\n",
      "[AGT] [INFO    ] [01:20:13] Starting epoch 4\n",
      "[AGT] [INFO    ] [01:20:13] Batch 4. Loss (accuracy): 0.837 <= 0.844 <= 0.852\n",
      "[AGT] [INFO    ] [01:21:56] Starting epoch 5\n",
      "[AGT] [INFO    ] [01:21:57] Batch 5. Loss (accuracy): 0.836 <= 0.844 <= 0.852\n",
      "[AGT] [INFO    ] [01:23:40] Starting epoch 6\n",
      "[AGT] [INFO    ] [01:23:40] Batch 6. Loss (accuracy): 0.833 <= 0.843 <= 0.852\n",
      "[AGT] [INFO    ] [01:25:24] Starting epoch 7\n",
      "[AGT] [INFO    ] [01:25:24] Batch 7. Loss (accuracy): 0.830 <= 0.842 <= 0.853\n",
      "[AGT] [INFO    ] [01:27:07] Starting epoch 8\n",
      "[AGT] [INFO    ] [01:27:07] Batch 8. Loss (accuracy): 0.829 <= 0.842 <= 0.851\n",
      "[AGT] [INFO    ] [01:28:51] Starting epoch 9\n",
      "[AGT] [INFO    ] [01:28:51] Batch 9. Loss (accuracy): 0.828 <= 0.839 <= 0.852\n",
      "[AGT] [INFO    ] [01:30:34] Starting epoch 10\n",
      "[AGT] [INFO    ] [01:30:34] Batch 10. Loss (accuracy): 0.827 <= 0.837 <= 0.853\n",
      "[AGT] [INFO    ] [01:32:18] Final Eval. Loss (accuracy): 0.824 <= 0.838 <= 0.852\n",
      "[AGT] [INFO    ] [01:32:18] =================== Finished Privacy Certified Training ===================\n",
      "[AGT] [INFO    ] [01:32:18] =================== Starting Privacy Certified Training ===================\n",
      "[AGT] [INFO    ] [01:32:18] Starting epoch 1\n",
      "[AGT] [INFO    ] [01:32:18] Batch 1. Loss (accuracy): 0.514 <= 0.514 <= 0.514\n",
      "[AGT] [INFO    ] [01:34:01] Dataloader has only one batch per epoch, effective batchsize may be smaller than expected.\n",
      "[AGT] [INFO    ] [01:34:01] Starting epoch 2\n",
      "[AGT] [INFO    ] [01:34:01] Batch 2. Loss (accuracy): 0.840 <= 0.849 <= 0.858\n",
      "[AGT] [INFO    ] [01:35:45] Starting epoch 3\n",
      "[AGT] [INFO    ] [01:35:45] Batch 3. Loss (accuracy): 0.833 <= 0.847 <= 0.858\n",
      "[AGT] [INFO    ] [01:37:29] Starting epoch 4\n",
      "[AGT] [INFO    ] [01:37:29] Batch 4. Loss (accuracy): 0.831 <= 0.844 <= 0.859\n",
      "[AGT] [INFO    ] [01:39:12] Starting epoch 5\n",
      "[AGT] [INFO    ] [01:39:12] Batch 5. Loss (accuracy): 0.829 <= 0.844 <= 0.859\n",
      "[AGT] [INFO    ] [01:40:56] Starting epoch 6\n",
      "[AGT] [INFO    ] [01:40:56] Batch 6. Loss (accuracy): 0.822 <= 0.843 <= 0.861\n",
      "[AGT] [INFO    ] [01:42:39] Starting epoch 7\n",
      "[AGT] [INFO    ] [01:42:39] Batch 7. Loss (accuracy): 0.819 <= 0.842 <= 0.860\n",
      "[AGT] [INFO    ] [01:44:23] Starting epoch 8\n",
      "[AGT] [INFO    ] [01:44:23] Batch 8. Loss (accuracy): 0.818 <= 0.842 <= 0.862\n",
      "[AGT] [INFO    ] [01:46:06] Starting epoch 9\n",
      "[AGT] [INFO    ] [01:46:06] Batch 9. Loss (accuracy): 0.815 <= 0.839 <= 0.864\n",
      "[AGT] [INFO    ] [01:47:50] Starting epoch 10\n",
      "[AGT] [INFO    ] [01:47:50] Batch 10. Loss (accuracy): 0.812 <= 0.837 <= 0.865\n",
      "[AGT] [INFO    ] [01:49:33] Final Eval. Loss (accuracy): 0.811 <= 0.838 <= 0.866\n",
      "[AGT] [INFO    ] [01:49:33] =================== Finished Privacy Certified Training ===================\n",
      "[AGT] [INFO    ] [01:49:33] =================== Starting Privacy Certified Training ===================\n",
      "[AGT] [INFO    ] [01:49:33] Starting epoch 1\n",
      "[AGT] [INFO    ] [01:49:33] Batch 1. Loss (accuracy): 0.514 <= 0.514 <= 0.514\n",
      "[AGT] [INFO    ] [01:51:17] Dataloader has only one batch per epoch, effective batchsize may be smaller than expected.\n",
      "[AGT] [INFO    ] [01:51:17] Starting epoch 2\n",
      "[AGT] [INFO    ] [01:51:17] Batch 2. Loss (accuracy): 0.824 <= 0.849 <= 0.871\n",
      "[AGT] [INFO    ] [01:53:01] Starting epoch 3\n",
      "[AGT] [INFO    ] [01:53:01] Batch 3. Loss (accuracy): 0.817 <= 0.847 <= 0.874\n",
      "[AGT] [INFO    ] [01:54:44] Starting epoch 4\n",
      "[AGT] [INFO    ] [01:54:44] Batch 4. Loss (accuracy): 0.809 <= 0.844 <= 0.879\n",
      "[AGT] [INFO    ] [01:56:28] Starting epoch 5\n",
      "[AGT] [INFO    ] [01:56:28] Batch 5. Loss (accuracy): 0.800 <= 0.844 <= 0.880\n",
      "[AGT] [INFO    ] [01:58:11] Starting epoch 6\n",
      "[AGT] [INFO    ] [01:58:11] Batch 6. Loss (accuracy): 0.793 <= 0.843 <= 0.886\n",
      "[AGT] [INFO    ] [01:59:54] Starting epoch 7\n",
      "[AGT] [INFO    ] [01:59:54] Batch 7. Loss (accuracy): 0.786 <= 0.842 <= 0.889\n",
      "[AGT] [INFO    ] [02:01:37] Starting epoch 8\n",
      "[AGT] [INFO    ] [02:01:37] Batch 8. Loss (accuracy): 0.781 <= 0.842 <= 0.891\n",
      "[AGT] [INFO    ] [02:03:19] Starting epoch 9\n",
      "[AGT] [INFO    ] [02:03:19] Batch 9. Loss (accuracy): 0.775 <= 0.839 <= 0.894\n",
      "[AGT] [INFO    ] [02:05:02] Starting epoch 10\n",
      "[AGT] [INFO    ] [02:05:02] Batch 10. Loss (accuracy): 0.767 <= 0.837 <= 0.898\n",
      "[AGT] [INFO    ] [02:06:45] Final Eval. Loss (accuracy): 0.762 <= 0.838 <= 0.903\n",
      "[AGT] [INFO    ] [02:06:45] =================== Finished Privacy Certified Training ===================\n",
      "[AGT] [INFO    ] [02:06:45] =================== Starting Privacy Certified Training ===================\n",
      "[AGT] [INFO    ] [02:06:45] Starting epoch 1\n",
      "[AGT] [INFO    ] [02:06:45] Batch 1. Loss (accuracy): 0.514 <= 0.514 <= 0.514\n",
      "[AGT] [INFO    ] [02:08:28] Dataloader has only one batch per epoch, effective batchsize may be smaller than expected.\n",
      "[AGT] [INFO    ] [02:08:28] Starting epoch 2\n",
      "[AGT] [INFO    ] [02:08:28] Batch 2. Loss (accuracy): 0.799 <= 0.849 <= 0.889\n",
      "[AGT] [INFO    ] [02:10:11] Starting epoch 3\n",
      "[AGT] [INFO    ] [02:10:11] Batch 3. Loss (accuracy): 0.783 <= 0.847 <= 0.897\n",
      "[AGT] [INFO    ] [02:11:54] Starting epoch 4\n",
      "[AGT] [INFO    ] [02:11:55] Batch 4. Loss (accuracy): 0.768 <= 0.844 <= 0.906\n",
      "[AGT] [INFO    ] [02:13:38] Starting epoch 5\n",
      "[AGT] [INFO    ] [02:13:38] Batch 5. Loss (accuracy): 0.752 <= 0.844 <= 0.913\n",
      "[AGT] [INFO    ] [02:15:21] Starting epoch 6\n",
      "[AGT] [INFO    ] [02:15:21] Batch 6. Loss (accuracy): 0.734 <= 0.843 <= 0.919\n",
      "[AGT] [INFO    ] [02:17:04] Starting epoch 7\n",
      "[AGT] [INFO    ] [02:17:04] Batch 7. Loss (accuracy): 0.717 <= 0.842 <= 0.929\n",
      "[AGT] [INFO    ] [02:18:47] Starting epoch 8\n",
      "[AGT] [INFO    ] [02:18:48] Batch 8. Loss (accuracy): 0.705 <= 0.842 <= 0.937\n",
      "[AGT] [INFO    ] [02:20:31] Starting epoch 9\n",
      "[AGT] [INFO    ] [02:20:31] Batch 9. Loss (accuracy): 0.692 <= 0.839 <= 0.946\n",
      "[AGT] [INFO    ] [02:22:14] Starting epoch 10\n",
      "[AGT] [INFO    ] [02:22:14] Batch 10. Loss (accuracy): 0.683 <= 0.837 <= 0.955\n",
      "[AGT] [INFO    ] [02:23:57] Final Eval. Loss (accuracy): 0.671 <= 0.838 <= 0.966\n",
      "[AGT] [INFO    ] [02:23:57] =================== Finished Privacy Certified Training ===================\n",
      "[AGT] [INFO    ] [02:23:57] =================== Starting Privacy Certified Training ===================\n",
      "[AGT] [INFO    ] [02:23:57] Starting epoch 1\n",
      "[AGT] [INFO    ] [02:23:57] Batch 1. Loss (accuracy): 0.514 <= 0.514 <= 0.514\n",
      "[AGT] [INFO    ] [02:25:41] Dataloader has only one batch per epoch, effective batchsize may be smaller than expected.\n",
      "[AGT] [INFO    ] [02:25:41] Starting epoch 2\n",
      "[AGT] [INFO    ] [02:25:41] Batch 2. Loss (accuracy): 0.672 <= 0.849 <= 0.916\n",
      "[AGT] [INFO    ] [02:27:24] Starting epoch 3\n",
      "[AGT] [INFO    ] [02:27:24] Batch 3. Loss (accuracy): 0.625 <= 0.847 <= 0.930\n",
      "[AGT] [INFO    ] [02:29:07] Starting epoch 4\n",
      "[AGT] [INFO    ] [02:29:07] Batch 4. Loss (accuracy): 0.588 <= 0.844 <= 0.947\n",
      "[AGT] [INFO    ] [02:30:50] Starting epoch 5\n",
      "[AGT] [INFO    ] [02:30:50] Batch 5. Loss (accuracy): 0.547 <= 0.844 <= 0.963\n",
      "[AGT] [INFO    ] [02:32:35] Starting epoch 6\n",
      "[AGT] [INFO    ] [02:32:35] Batch 6. Loss (accuracy): 0.500 <= 0.843 <= 0.979\n",
      "[AGT] [INFO    ] [02:34:19] Starting epoch 7\n",
      "[AGT] [INFO    ] [02:34:19] Batch 7. Loss (accuracy): 0.437 <= 0.842 <= 0.992\n",
      "[AGT] [INFO    ] [02:36:04] Starting epoch 8\n",
      "[AGT] [INFO    ] [02:36:04] Batch 8. Loss (accuracy): 0.364 <= 0.842 <= 0.998\n",
      "[AGT] [INFO    ] [02:37:48] Starting epoch 9\n",
      "[AGT] [INFO    ] [02:37:48] Batch 9. Loss (accuracy): 0.273 <= 0.839 <= 1.000\n",
      "[AGT] [INFO    ] [02:39:32] Starting epoch 10\n",
      "[AGT] [INFO    ] [02:39:33] Batch 10. Loss (accuracy): 0.196 <= 0.837 <= 1.000\n",
      "[AGT] [INFO    ] [02:41:17] Final Eval. Loss (accuracy): 0.167 <= 0.838 <= 1.000\n",
      "[AGT] [INFO    ] [02:41:17] =================== Finished Privacy Certified Training ===================\n",
      "[AGT] [INFO    ] [02:41:17] =================== Starting Privacy Certified Training ===================\n",
      "[AGT] [INFO    ] [02:41:17] Starting epoch 1\n",
      "[AGT] [INFO    ] [02:41:17] Batch 1. Loss (accuracy): 0.514 <= 0.514 <= 0.514\n",
      "[AGT] [INFO    ] [02:43:02] Dataloader has only one batch per epoch, effective batchsize may be smaller than expected.\n",
      "[AGT] [INFO    ] [02:43:02] Starting epoch 2\n",
      "[AGT] [INFO    ] [02:43:02] Batch 2. Loss (accuracy): 0.233 <= 0.849 <= 0.984\n",
      "[AGT] [INFO    ] [02:44:46] Starting epoch 3\n",
      "[AGT] [INFO    ] [02:44:47] Batch 3. Loss (accuracy): 0.191 <= 0.847 <= 1.000\n",
      "[AGT] [INFO    ] [02:46:30] Starting epoch 4\n",
      "[AGT] [INFO    ] [02:46:30] Batch 4. Loss (accuracy): 0.025 <= 0.844 <= 1.000\n",
      "[AGT] [INFO    ] [02:48:14] Starting epoch 5\n",
      "[AGT] [INFO    ] [02:48:14] Batch 5. Loss (accuracy): 0.000 <= 0.844 <= 1.000\n",
      "[AGT] [INFO    ] [02:49:59] Starting epoch 6\n",
      "[AGT] [INFO    ] [02:49:59] Batch 6. Loss (accuracy): 0.000 <= 0.843 <= 1.000\n",
      "[AGT] [INFO    ] [02:51:44] Starting epoch 7\n",
      "[AGT] [INFO    ] [02:51:44] Batch 7. Loss (accuracy): 0.000 <= 0.842 <= 1.000\n",
      "[AGT] [INFO    ] [02:53:28] Starting epoch 8\n",
      "[AGT] [INFO    ] [02:53:28] Batch 8. Loss (accuracy): 0.000 <= 0.842 <= 1.000\n",
      "[AGT] [INFO    ] [02:55:11] Starting epoch 9\n",
      "[AGT] [INFO    ] [02:55:12] Batch 9. Loss (accuracy): 0.000 <= 0.839 <= 1.000\n",
      "[AGT] [INFO    ] [02:56:54] Starting epoch 10\n",
      "[AGT] [INFO    ] [02:56:55] Batch 10. Loss (accuracy): 0.000 <= 0.837 <= 1.000\n",
      "[AGT] [INFO    ] [02:58:38] Final Eval. Loss (accuracy): 0.000 <= 0.838 <= 1.000\n",
      "[AGT] [INFO    ] [02:58:38] =================== Finished Privacy Certified Training ===================\n",
      "[AGT] [INFO    ] [02:58:38] =================== Starting Privacy Certified Training ===================\n",
      "[AGT] [INFO    ] [02:58:38] Starting epoch 1\n",
      "[AGT] [INFO    ] [02:58:38] Batch 1. Loss (accuracy): 0.514 <= 0.514 <= 0.514\n",
      "[AGT] [INFO    ] [03:00:22] Dataloader has only one batch per epoch, effective batchsize may be smaller than expected.\n",
      "[AGT] [INFO    ] [03:00:22] Starting epoch 2\n",
      "[AGT] [INFO    ] [03:00:22] Batch 2. Loss (accuracy): 0.000 <= 0.849 <= 1.000\n",
      "[AGT] [INFO    ] [03:02:05] Starting epoch 3\n",
      "[AGT] [INFO    ] [03:02:06] Batch 3. Loss (accuracy): 0.000 <= 0.847 <= 1.000\n",
      "[AGT] [INFO    ] [03:03:49] Starting epoch 4\n",
      "[AGT] [INFO    ] [03:03:49] Batch 4. Loss (accuracy): 0.000 <= 0.844 <= 1.000\n",
      "[AGT] [INFO    ] [03:05:33] Starting epoch 5\n",
      "[AGT] [INFO    ] [03:05:33] Batch 5. Loss (accuracy): 0.000 <= 0.844 <= 1.000\n",
      "[AGT] [INFO    ] [03:07:17] Starting epoch 6\n",
      "[AGT] [INFO    ] [03:07:17] Batch 6. Loss (accuracy): 0.000 <= 0.843 <= 1.000\n",
      "[AGT] [INFO    ] [03:09:01] Starting epoch 7\n",
      "[AGT] [INFO    ] [03:09:01] Batch 7. Loss (accuracy): 0.000 <= 0.842 <= 1.000\n",
      "[AGT] [INFO    ] [03:10:45] Starting epoch 8\n",
      "[AGT] [INFO    ] [03:10:46] Batch 8. Loss (accuracy): 0.000 <= 0.842 <= 1.000\n",
      "[AGT] [INFO    ] [03:12:29] Starting epoch 9\n",
      "[AGT] [INFO    ] [03:12:30] Batch 9. Loss (accuracy): 0.000 <= 0.839 <= 1.000\n",
      "[AGT] [INFO    ] [03:14:13] Starting epoch 10\n",
      "[AGT] [INFO    ] [03:14:13] Batch 10. Loss (accuracy): 0.000 <= 0.837 <= 1.000\n",
      "[AGT] [INFO    ] [03:15:57] Final Eval. Loss (accuracy): 0.000 <= 0.838 <= 1.000\n",
      "[AGT] [INFO    ] [03:15:57] =================== Finished Privacy Certified Training ===================\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(agt)\n",
    "\"\"\"Let's train a logistic classifier on the halfmoons example above.\"\"\"\n",
    "# model = torch.nn.Sequential(torch.nn.Linear(7, 2))\n",
    "torch.manual_seed(1)\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(7, 128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, 2),\n",
    ")\n",
    "config = agt.AGTConfig(\n",
    "    fragsize=2000,\n",
    "    learning_rate=0.5,\n",
    "    n_epochs=10,\n",
    "    device=\"cuda:0\",\n",
    "    l2_reg=0.01,\n",
    "    k_private=1,\n",
    "    loss=\"cross_entropy\",\n",
    "    log_level=\"INFO\",\n",
    "    lr_decay=2.0,\n",
    "    clip_gamma=1.0,\n",
    "    lr_min=0.001,\n",
    "    optimizer=\"SGDM\", # we'll use SGD with momentum\n",
    "    optimizer_kwargs={\"momentum\": 0.9, \"nesterov\": True},\n",
    ")\n",
    "# k_values = [1,2,5,10,20,50,100]  # using more values here will improve the guarantees AGT will give\n",
    "k_values = [0]  # using more values here will improve the guarantees AGT will give\n",
    "bounded_model_dict = {}  # we'll store our results for each value of 'k' as a dictionary from 'k' to the bounded model\n",
    "\n",
    "for k_private in k_values:\n",
    "    config.k_private=k_private\n",
    "    torch.manual_seed(seed)\n",
    "    bounded_model = agt.bounded_models.IntervalBoundedModel(model)\n",
    "    bounded_model = agt.privacy_certified_training_user_level(bounded_model, config, dataloader_train, dataloader_test)\n",
    "    bounded_model_dict[k_private] = bounded_model\n",
    "    \n",
    "    # as a metric, compute the number of predictions in the test set certified at this value of k_private\n",
    "    certified_preds = agt.test_metrics.certified_predictions(bounded_model, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87a2ea68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise Free Accuracy: 0.82\n",
      "Smooth Sensitivity Accuracy: 0.50\n",
      "Global Sensitivity Accuracy: 0.54\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Let's use this set of bounded models to for better private prediction using the smooth sensitivity mechanism.\"\"\"\n",
    "\n",
    "epsilon = 0.2  # privacy loss\n",
    "noise_free_acc = agt.test_metrics.test_accuracy(bounded_model_dict[1], x_test, y_test)[0]\n",
    "\n",
    "# compute accuracy using the smooth sensitivity Cauchy mechanism\n",
    "smooth_sens_noise_level = agt.privacy_utils.get_calibrated_noise_level(\n",
    "    x_test, bounded_model_dict, epsilon, noise_type=\"cauchy\"\n",
    ")\n",
    "smooth_sens_acc = agt.privacy_utils.noisy_test_accuracy(\n",
    "    bounded_model_dict[1], x_test, y_test, noise_level=smooth_sens_noise_level, noise_type=\"cauchy\"\n",
    ")\n",
    "\n",
    "\n",
    "# compute accuracy when using the global sensitivity mechanism\n",
    "global_sens_acc = agt.privacy_utils.noisy_test_accuracy(\n",
    "    bounded_model_dict[1], x_test, y_test, noise_level=1.0 / epsilon\n",
    ")\n",
    "\n",
    "print(f\"Noise Free Accuracy: {noise_free_acc:.2f}\")\n",
    "print(f\"Smooth Sensitivity Accuracy: {smooth_sens_acc:.2f}\")\n",
    "print(f\"Global Sensitivity Accuracy: {global_sens_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60807be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [1]:\n",
    "    bounded_model_dict[k].save_params(f\"model_user_10000/1000/k_{k}.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44a5e7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Initialise a large model (which will be random here but would be a pre-trained model in practice).\"\"\"\n",
    "# model = torch.nn.Sequential(\n",
    "#     torch.nn.Linear(7, 128),\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Linear(128, 128),\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Linear(128, 128),\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Linear(128, 2)\n",
    "# )\n",
    "# config = agt.AGTConfig(\n",
    "#     learning_rate=0.5,\n",
    "#     n_epochs=2,\n",
    "#     loss=\"cross_entropy\",\n",
    "#     log_level=\"INFO\",\n",
    "#     device=\"cuda:0\",\n",
    "#     clip_gamma=0.1,\n",
    "#     k_private=10\n",
    "# )\n",
    "\n",
    "# # first try training the whole thing - observe that the certified accuracy goes to zero\n",
    "# bounded_model = agt.bounded_models.IntervalBoundedModel(model)\n",
    "# bounded_model = agt.privacy_certified_training_user_level(bounded_model, config, dataloader_train, dataloader_test)\n",
    "\n",
    "# # second, split the model into a fixed part and a trainable part\n",
    "# fixed_layers, trainable_layers = model[:4], model[4:]\n",
    "# # wrap both in bounded models, using the first as the 'transform' argument to the second\n",
    "# transform = agt.bounded_models.IntervalBoundedModel(fixed_layers, trainable=False)\n",
    "# bounded_model = agt.bounded_models.IntervalBoundedModel(trainable_layers, transform=transform)\n",
    "# # train the model\n",
    "# bounded_model = agt.privacy_certified_training_user_level(bounded_model, config, dataloader_train, dataloader_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
