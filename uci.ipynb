{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afc5ea04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x743f39908df0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import copy\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import tqdm\n",
    "import torchvision\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "import abstract_gradient_training as agt\n",
    "from abstract_gradient_training import AGTConfig\n",
    "from abstract_gradient_training.bounded_models import IntervalBoundedModel\n",
    "\n",
    "import uci_datasets  # python -m pip install git+https://github.com/treforevans/uci_datasets.git\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1fa1ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "houseelectric dataset, N=2049280, d=11\n",
      "<uci_datasets.dataset.Dataset object at 0x743d7a902590>\n"
     ]
    }
   ],
   "source": [
    "batchsize = 1000000\n",
    "data = uci_datasets.Dataset(\"houseelectric\")\n",
    "print(data)\n",
    "x_train, y_train, x_test, y_test = data.get_split(split=0)\n",
    "\n",
    "# Normalise the features and labels\n",
    "x_train_mu, x_train_std = x_train.mean(axis=0), x_train.std(axis=0)\n",
    "x_train = (x_train - x_train_mu) / x_train_std\n",
    "x_test = (x_test - x_train_mu) / x_train_std\n",
    "y_train_min, y_train_range = y_train.min(axis=0), y_train.max(axis=0) - y_train.min(axis=0)\n",
    "y_train = (y_train - y_train_min) / y_train_range\n",
    "y_test = (y_test - y_train_min) / y_train_range\n",
    "\n",
    "# Form datasets and dataloaders\n",
    "train_data = torch.utils.data.TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train).float())\n",
    "test_data = torch.utils.data.TensorDataset(torch.from_numpy(x_test).float(), torch.from_numpy(y_test).float())\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batchsize, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1000, shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "061ff025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batchsize = 1000000\n",
    "# # configure the training parameters\n",
    "# nominal_config = agt.AGTConfig(\n",
    "#     fragsize=20000,\n",
    "#     learning_rate=0.005,\n",
    "#     epsilon=0.01,\n",
    "#     k_private=1,\n",
    "#     n_epochs=1,\n",
    "#     device=\"cuda:0\",\n",
    "#     loss=\"mse\",\n",
    "#     log_level=\"DEBUG\",\n",
    "#     optimizer=\"SGDM\", # we'll use SGD with momentum\n",
    "#     optimizer_kwargs={\"momentum\": 0.9, \"nesterov\": True},\n",
    "# )\n",
    "\n",
    "# set up the AGT configuration\n",
    "nominal_config = AGTConfig(\n",
    "    fragsize=2000,\n",
    "    learning_rate=0.25,\n",
    "    n_epochs=50,\n",
    "    device=\"cuda:1\",\n",
    "    l2_reg=0.01,\n",
    "    k_private=1,\n",
    "    loss=\"mse\",\n",
    "    log_level=\"INFO\",\n",
    "    lr_decay=2.0,\n",
    "    clip_gamma=1.0,\n",
    "    lr_min=0.001,\n",
    "    optimizer=\"SGDM\", # we'll use SGD with momentum\n",
    "    optimizer_kwargs={\"momentum\": 0.9, \"nesterov\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a402b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[AGT] [INFO    ] [12:00:07] =================== Starting Privacy Certified Training ===================\n",
      "[AGT] [INFO    ] [12:00:07] Starting epoch 1\n",
      "[AGT] [INFO    ] [12:00:23] Batch 1. Loss (mse): 0.340 <= 0.340 <= 0.340\n",
      "[AGT] [INFO    ] [12:02:33] Starting epoch 2\n",
      "[AGT] [INFO    ] [12:02:47] Batch 2. Loss (mse): 2.222 <= 2.222 <= 2.222\n",
      "[AGT] [INFO    ] [12:05:01] Starting epoch 3\n",
      "[AGT] [INFO    ] [12:05:14] Batch 3. Loss (mse): 0.249 <= 0.249 <= 0.249\n",
      "[AGT] [INFO    ] [12:07:23] Starting epoch 4\n",
      "[AGT] [INFO    ] [12:07:37] Batch 4. Loss (mse): 0.053 <= 0.053 <= 0.053\n",
      "[AGT] [INFO    ] [12:09:50] Starting epoch 5\n",
      "[AGT] [INFO    ] [12:10:04] Batch 5. Loss (mse): 0.076 <= 0.076 <= 0.076\n",
      "[AGT] [INFO    ] [12:12:14] Starting epoch 6\n",
      "[AGT] [INFO    ] [12:12:28] Batch 6. Loss (mse): 0.080 <= 0.080 <= 0.081\n",
      "[AGT] [INFO    ] [12:14:38] Starting epoch 7\n",
      "[AGT] [INFO    ] [12:14:52] Batch 7. Loss (mse): 0.061 <= 0.062 <= 0.062\n",
      "[AGT] [INFO    ] [12:17:03] Starting epoch 8\n",
      "[AGT] [INFO    ] [12:17:16] Batch 8. Loss (mse): 0.044 <= 0.045 <= 0.045\n",
      "[AGT] [INFO    ] [12:19:30] Starting epoch 9\n",
      "[AGT] [INFO    ] [12:19:44] Batch 9. Loss (mse): 0.029 <= 0.030 <= 0.030\n",
      "[AGT] [INFO    ] [12:21:49] Starting epoch 10\n",
      "[AGT] [INFO    ] [12:22:03] Batch 10. Loss (mse): 0.023 <= 0.024 <= 0.024\n",
      "[AGT] [INFO    ] [12:24:13] Starting epoch 11\n",
      "[AGT] [INFO    ] [12:24:27] Batch 11. Loss (mse): 0.025 <= 0.026 <= 0.027\n",
      "[AGT] [INFO    ] [12:26:39] Starting epoch 12\n",
      "[AGT] [INFO    ] [12:26:50] Batch 12. Loss (mse): 0.019 <= 0.020 <= 0.021\n",
      "[AGT] [INFO    ] [12:29:08] Starting epoch 13\n",
      "[AGT] [INFO    ] [12:29:22] Batch 13. Loss (mse): 0.016 <= 0.017 <= 0.019\n",
      "[AGT] [INFO    ] [12:31:34] Starting epoch 14\n",
      "[AGT] [INFO    ] [12:31:48] Batch 14. Loss (mse): 0.022 <= 0.024 <= 0.027\n",
      "[AGT] [INFO    ] [12:33:59] Starting epoch 15\n",
      "[AGT] [INFO    ] [12:34:13] Batch 15. Loss (mse): 0.015 <= 0.017 <= 0.019\n",
      "[AGT] [INFO    ] [12:36:28] Starting epoch 16\n",
      "[AGT] [INFO    ] [12:36:42] Batch 16. Loss (mse): 0.017 <= 0.021 <= 0.025\n",
      "[AGT] [INFO    ] [12:38:56] Starting epoch 17\n",
      "[AGT] [INFO    ] [12:39:09] Batch 17. Loss (mse): 0.013 <= 0.017 <= 0.021\n",
      "[AGT] [INFO    ] [12:41:19] Starting epoch 18\n",
      "[AGT] [INFO    ] [12:41:33] Batch 18. Loss (mse): 0.011 <= 0.015 <= 0.020\n",
      "[AGT] [INFO    ] [12:43:37] Starting epoch 19\n",
      "[AGT] [INFO    ] [12:43:51] Batch 19. Loss (mse): 0.010 <= 0.014 <= 0.021\n",
      "[AGT] [INFO    ] [12:45:59] Starting epoch 20\n",
      "[AGT] [INFO    ] [12:46:13] Batch 20. Loss (mse): 0.010 <= 0.016 <= 0.025\n",
      "[AGT] [INFO    ] [12:48:27] Starting epoch 21\n",
      "[AGT] [INFO    ] [12:48:41] Batch 21. Loss (mse): 0.006 <= 0.012 <= 0.022\n",
      "[AGT] [INFO    ] [12:51:01] Starting epoch 22\n",
      "[AGT] [INFO    ] [12:51:15] Batch 22. Loss (mse): 0.005 <= 0.011 <= 0.022\n",
      "[AGT] [INFO    ] [12:53:24] Starting epoch 23\n",
      "[AGT] [INFO    ] [12:53:39] Batch 23. Loss (mse): 0.005 <= 0.013 <= 0.030\n",
      "[AGT] [INFO    ] [12:55:51] Starting epoch 24\n",
      "[AGT] [INFO    ] [12:56:05] Batch 24. Loss (mse): 0.003 <= 0.012 <= 0.033\n",
      "[AGT] [INFO    ] [12:58:18] Starting epoch 25\n",
      "[AGT] [INFO    ] [12:58:33] Batch 25. Loss (mse): 0.003 <= 0.013 <= 0.041\n",
      "[AGT] [INFO    ] [13:00:46] Starting epoch 26\n",
      "[AGT] [INFO    ] [13:01:00] Batch 26. Loss (mse): 0.002 <= 0.011 <= 0.046\n",
      "[AGT] [INFO    ] [13:03:16] Starting epoch 27\n",
      "[AGT] [INFO    ] [13:03:30] Batch 27. Loss (mse): 0.001 <= 0.012 <= 0.060\n",
      "[AGT] [INFO    ] [13:05:42] Starting epoch 28\n",
      "[AGT] [INFO    ] [13:05:55] Batch 28. Loss (mse): 0.001 <= 0.011 <= 0.071\n",
      "[AGT] [INFO    ] [13:08:07] Starting epoch 29\n",
      "[AGT] [INFO    ] [13:08:20] Batch 29. Loss (mse): 0.000 <= 0.012 <= 0.092\n",
      "[AGT] [INFO    ] [13:10:31] Starting epoch 30\n",
      "[AGT] [INFO    ] [13:10:44] Batch 30. Loss (mse): 0.000 <= 0.011 <= 0.113\n",
      "[AGT] [INFO    ] [13:12:58] Starting epoch 31\n",
      "[AGT] [INFO    ] [13:13:12] Batch 31. Loss (mse): 0.000 <= 0.010 <= 0.142\n",
      "[AGT] [INFO    ] [13:15:24] Starting epoch 32\n",
      "[AGT] [INFO    ] [13:15:38] Batch 32. Loss (mse): 0.000 <= 0.012 <= 0.203\n",
      "[AGT] [INFO    ] [13:17:52] Starting epoch 33\n",
      "[AGT] [INFO    ] [13:18:08] Batch 33. Loss (mse): 0.000 <= 0.012 <= 0.257\n",
      "[AGT] [INFO    ] [13:20:19] Starting epoch 34\n",
      "[AGT] [INFO    ] [13:20:33] Batch 34. Loss (mse): 0.000 <= 0.011 <= 0.327\n",
      "[AGT] [INFO    ] [13:22:53] Starting epoch 35\n",
      "[AGT] [INFO    ] [13:23:07] Batch 35. Loss (mse): 0.000 <= 0.012 <= 0.431\n",
      "[AGT] [INFO    ] [13:25:22] Starting epoch 36\n",
      "[AGT] [INFO    ] [13:25:36] Batch 36. Loss (mse): 0.000 <= 0.011 <= 0.593\n",
      "[AGT] [INFO    ] [13:27:46] Starting epoch 37\n",
      "[AGT] [INFO    ] [13:28:00] Batch 37. Loss (mse): 0.000 <= 0.010 <= 0.721\n",
      "[AGT] [INFO    ] [13:30:14] Starting epoch 38\n",
      "[AGT] [INFO    ] [13:30:28] Batch 38. Loss (mse): 0.000 <= 0.010 <= 0.958\n",
      "[AGT] [INFO    ] [13:32:43] Starting epoch 39\n",
      "[AGT] [INFO    ] [13:32:56] Batch 39. Loss (mse): 0.000 <= 0.011 <= 1.253\n",
      "[AGT] [INFO    ] [13:35:06] Starting epoch 40\n",
      "[AGT] [INFO    ] [13:35:20] Batch 40. Loss (mse): 0.000 <= 0.010 <= 1.653\n",
      "[AGT] [INFO    ] [13:37:44] Starting epoch 41\n",
      "[AGT] [INFO    ] [13:37:58] Batch 41. Loss (mse): 0.000 <= 0.010 <= 2.171\n",
      "[AGT] [INFO    ] [13:40:19] Starting epoch 42\n",
      "[AGT] [INFO    ] [13:40:33] Batch 42. Loss (mse): 0.000 <= 0.011 <= 3.024\n",
      "[AGT] [INFO    ] [13:42:46] Starting epoch 43\n",
      "[AGT] [INFO    ] [13:43:00] Batch 43. Loss (mse): 0.000 <= 0.011 <= 4.015\n",
      "[AGT] [INFO    ] [13:45:24] Starting epoch 44\n",
      "[AGT] [INFO    ] [13:45:37] Batch 44. Loss (mse): 0.000 <= 0.010 <= 5.426\n",
      "[AGT] [INFO    ] [13:47:52] Starting epoch 45\n",
      "[AGT] [INFO    ] [13:48:06] Batch 45. Loss (mse): 0.000 <= 0.011 <= 7.462\n",
      "[AGT] [INFO    ] [13:50:23] Starting epoch 46\n",
      "[AGT] [INFO    ] [13:50:37] Batch 46. Loss (mse): 0.000 <= 0.010 <= 10.437\n",
      "[AGT] [INFO    ] [13:52:53] Starting epoch 47\n",
      "[AGT] [INFO    ] [13:53:08] Batch 47. Loss (mse): 0.000 <= 0.011 <= 15.484\n",
      "[AGT] [INFO    ] [13:55:20] Starting epoch 48\n",
      "[AGT] [INFO    ] [13:55:34] Batch 48. Loss (mse): 0.000 <= 0.011 <= 20.912\n",
      "[AGT] [INFO    ] [13:57:59] Starting epoch 49\n",
      "[AGT] [INFO    ] [13:58:13] Batch 49. Loss (mse): 0.000 <= 0.011 <= 29.851\n",
      "[AGT] [INFO    ] [14:00:31] Starting epoch 50\n",
      "[AGT] [INFO    ] [14:00:46] Batch 50. Loss (mse): 0.000 <= 0.010 <= 44.867\n",
      "[AGT] [INFO    ] [14:03:00] Final Eval. Loss (mse): 0.000 <= 0.012 <= 64.725\n",
      "[AGT] [INFO    ] [14:03:00] =================== Finished Privacy Certified Training ===================\n",
      " 14%|█▍        | 1/7 [2:02:52<12:17:17, 7372.99s/it][AGT] [INFO    ] [14:03:00] =================== Starting Privacy Certified Training ===================\n",
      "[AGT] [INFO    ] [14:03:00] Starting epoch 1\n",
      "[AGT] [INFO    ] [14:03:14] Batch 1. Loss (mse): 0.340 <= 0.340 <= 0.340\n",
      "[AGT] [INFO    ] [14:05:33] Starting epoch 2\n",
      "[AGT] [INFO    ] [14:05:47] Batch 2. Loss (mse): 2.222 <= 2.222 <= 2.222\n",
      "[AGT] [INFO    ] [14:08:05] Starting epoch 3\n",
      "[AGT] [INFO    ] [14:08:18] Batch 3. Loss (mse): 0.249 <= 0.249 <= 0.249\n",
      "[AGT] [INFO    ] [14:10:33] Starting epoch 4\n",
      "[AGT] [INFO    ] [14:10:47] Batch 4. Loss (mse): 0.053 <= 0.053 <= 0.053\n",
      "[AGT] [INFO    ] [14:13:04] Starting epoch 5\n",
      "[AGT] [INFO    ] [14:13:16] Batch 5. Loss (mse): 0.075 <= 0.076 <= 0.076\n",
      "[AGT] [INFO    ] [14:15:20] Starting epoch 6\n",
      "[AGT] [INFO    ] [14:15:34] Batch 6. Loss (mse): 0.080 <= 0.080 <= 0.081\n",
      "[AGT] [INFO    ] [14:17:49] Starting epoch 7\n",
      "[AGT] [INFO    ] [14:18:03] Batch 7. Loss (mse): 0.061 <= 0.062 <= 0.063\n",
      "[AGT] [INFO    ] [14:20:18] Starting epoch 8\n",
      "[AGT] [INFO    ] [14:20:32] Batch 8. Loss (mse): 0.044 <= 0.045 <= 0.045\n",
      "[AGT] [INFO    ] [14:22:52] Starting epoch 9\n",
      "[AGT] [INFO    ] [14:23:02] Batch 9. Loss (mse): 0.029 <= 0.030 <= 0.031\n",
      "[AGT] [INFO    ] [14:25:26] Starting epoch 10\n",
      "[AGT] [INFO    ] [14:25:40] Batch 10. Loss (mse): 0.022 <= 0.024 <= 0.025\n",
      "[AGT] [INFO    ] [14:28:01] Starting epoch 11\n",
      "[AGT] [INFO    ] [14:28:15] Batch 11. Loss (mse): 0.024 <= 0.026 <= 0.028\n",
      "[AGT] [INFO    ] [14:30:36] Starting epoch 12\n",
      "[AGT] [INFO    ] [14:30:50] Batch 12. Loss (mse): 0.018 <= 0.020 <= 0.023\n",
      "[AGT] [INFO    ] [14:33:02] Starting epoch 13\n",
      "[AGT] [INFO    ] [14:33:16] Batch 13. Loss (mse): 0.015 <= 0.017 <= 0.020\n",
      "[AGT] [INFO    ] [14:35:31] Starting epoch 14\n",
      "[AGT] [INFO    ] [14:35:45] Batch 14. Loss (mse): 0.020 <= 0.024 <= 0.029\n",
      "[AGT] [INFO    ] [14:38:05] Starting epoch 15\n",
      "[AGT] [INFO    ] [14:38:19] Batch 15. Loss (mse): 0.013 <= 0.017 <= 0.022\n",
      "[AGT] [INFO    ] [14:40:32] Starting epoch 16\n",
      "[AGT] [INFO    ] [14:40:46] Batch 16. Loss (mse): 0.015 <= 0.021 <= 0.029\n",
      "[AGT] [INFO    ] [14:42:58] Starting epoch 17\n",
      "[AGT] [INFO    ] [14:43:12] Batch 17. Loss (mse): 0.011 <= 0.017 <= 0.026\n",
      "[AGT] [INFO    ] [14:45:27] Starting epoch 18\n",
      "[AGT] [INFO    ] [14:45:40] Batch 18. Loss (mse): 0.008 <= 0.015 <= 0.026\n",
      "[AGT] [INFO    ] [14:47:46] Starting epoch 19\n",
      "[AGT] [INFO    ] [14:48:00] Batch 19. Loss (mse): 0.006 <= 0.014 <= 0.029\n",
      "[AGT] [INFO    ] [14:50:13] Starting epoch 20\n",
      "[AGT] [INFO    ] [14:50:28] Batch 20. Loss (mse): 0.006 <= 0.016 <= 0.038\n",
      "[AGT] [INFO    ] [14:52:42] Starting epoch 21\n",
      "[AGT] [INFO    ] [14:52:55] Batch 21. Loss (mse): 0.003 <= 0.012 <= 0.036\n",
      "[AGT] [INFO    ] [14:55:13] Starting epoch 22\n",
      "[AGT] [INFO    ] [14:55:27] Batch 22. Loss (mse): 0.002 <= 0.011 <= 0.040\n",
      "[AGT] [INFO    ] [14:57:39] Starting epoch 23\n",
      "[AGT] [INFO    ] [14:57:52] Batch 23. Loss (mse): 0.002 <= 0.013 <= 0.058\n",
      "[AGT] [INFO    ] [15:00:08] Starting epoch 24\n",
      "[AGT] [INFO    ] [15:00:21] Batch 24. Loss (mse): 0.001 <= 0.012 <= 0.071\n",
      "[AGT] [INFO    ] [15:02:43] Starting epoch 25\n",
      "[AGT] [INFO    ] [15:02:57] Batch 25. Loss (mse): 0.000 <= 0.013 <= 0.092\n",
      "[AGT] [INFO    ] [15:05:14] Starting epoch 26\n",
      "[AGT] [INFO    ] [15:05:28] Batch 26. Loss (mse): 0.000 <= 0.011 <= 0.114\n",
      "[AGT] [INFO    ] [15:07:38] Starting epoch 27\n",
      "[AGT] [INFO    ] [15:07:49] Batch 27. Loss (mse): 0.000 <= 0.012 <= 0.159\n",
      "[AGT] [INFO    ] [15:10:04] Starting epoch 28\n",
      "[AGT] [INFO    ] [15:10:17] Batch 28. Loss (mse): 0.000 <= 0.011 <= 0.199\n",
      "[AGT] [INFO    ] [15:12:27] Starting epoch 29\n",
      "[AGT] [INFO    ] [15:12:41] Batch 29. Loss (mse): 0.000 <= 0.012 <= 0.274\n",
      "[AGT] [INFO    ] [15:14:54] Starting epoch 30\n",
      "[AGT] [INFO    ] [15:15:07] Batch 30. Loss (mse): 0.000 <= 0.011 <= 0.360\n",
      "[AGT] [INFO    ] [15:17:17] Starting epoch 31\n",
      "[AGT] [INFO    ] [15:17:30] Batch 31. Loss (mse): 0.000 <= 0.010 <= 0.471\n",
      "[AGT] [INFO    ] [15:19:38] Starting epoch 32\n",
      "[AGT] [INFO    ] [15:19:50] Batch 32. Loss (mse): 0.000 <= 0.012 <= 0.696\n",
      "[AGT] [INFO    ] [15:22:00] Starting epoch 33\n",
      "[AGT] [INFO    ] [15:22:15] Batch 33. Loss (mse): 0.000 <= 0.012 <= 0.915\n",
      "[AGT] [INFO    ] [15:24:29] Starting epoch 34\n",
      "[AGT] [INFO    ] [15:24:42] Batch 34. Loss (mse): 0.000 <= 0.011 <= 1.211\n",
      "[AGT] [INFO    ] [15:26:57] Starting epoch 35\n",
      "[AGT] [INFO    ] [15:27:12] Batch 35. Loss (mse): 0.000 <= 0.012 <= 1.644\n",
      "[AGT] [INFO    ] [15:29:26] Starting epoch 36\n",
      "[AGT] [INFO    ] [15:29:40] Batch 36. Loss (mse): 0.000 <= 0.011 <= 2.346\n",
      "[AGT] [INFO    ] [15:31:54] Starting epoch 37\n",
      "[AGT] [INFO    ] [15:32:09] Batch 37. Loss (mse): 0.000 <= 0.010 <= 2.989\n",
      "[AGT] [INFO    ] [15:34:19] Starting epoch 38\n",
      "[AGT] [INFO    ] [15:34:34] Batch 38. Loss (mse): 0.000 <= 0.010 <= 4.144\n",
      "[AGT] [INFO    ] [15:36:53] Starting epoch 39\n",
      "[AGT] [INFO    ] [15:37:06] Batch 39. Loss (mse): 0.000 <= 0.011 <= 5.714\n",
      "[AGT] [INFO    ] [15:39:27] Starting epoch 40\n",
      "[AGT] [INFO    ] [15:39:40] Batch 40. Loss (mse): 0.000 <= 0.010 <= 8.112\n",
      "[AGT] [INFO    ] [15:41:58] Starting epoch 41\n",
      "[AGT] [INFO    ] [15:42:12] Batch 41. Loss (mse): 0.000 <= 0.010 <= 11.644\n",
      "[AGT] [INFO    ] [15:44:19] Starting epoch 42\n",
      "[AGT] [INFO    ] [15:44:32] Batch 42. Loss (mse): 0.000 <= 0.011 <= 17.673\n",
      "[AGT] [INFO    ] [15:46:41] Starting epoch 43\n",
      "[AGT] [INFO    ] [15:46:54] Batch 43. Loss (mse): 0.000 <= 0.011 <= 25.760\n",
      "[AGT] [INFO    ] [15:49:07] Starting epoch 44\n",
      "[AGT] [INFO    ] [15:49:21] Batch 44. Loss (mse): 0.000 <= 0.010 <= 38.668\n",
      "[AGT] [INFO    ] [15:51:42] Starting epoch 45\n",
      "[AGT] [INFO    ] [15:51:55] Batch 45. Loss (mse): 0.000 <= 0.011 <= 57.604\n",
      "[AGT] [INFO    ] [15:54:09] Starting epoch 46\n",
      "[AGT] [INFO    ] [15:54:23] Batch 46. Loss (mse): 0.000 <= 0.010 <= 87.066\n",
      "[AGT] [INFO    ] [15:56:29] Starting epoch 47\n",
      "[AGT] [INFO    ] [15:56:42] Batch 47. Loss (mse): 0.000 <= 0.011 <= 132.456\n",
      "[AGT] [INFO    ] [15:58:58] Starting epoch 48\n",
      "[AGT] [INFO    ] [15:59:11] Batch 48. Loss (mse): 0.000 <= 0.011 <= 186.486\n",
      "[AGT] [INFO    ] [16:01:25] Starting epoch 49\n",
      "[AGT] [INFO    ] [16:01:39] Batch 49. Loss (mse): 0.000 <= 0.011 <= 264.310\n",
      "[AGT] [INFO    ] [16:03:57] Starting epoch 50\n",
      "[AGT] [INFO    ] [16:04:10] Batch 50. Loss (mse): 0.000 <= 0.010 <= 387.023\n",
      "[AGT] [INFO    ] [16:06:25] Final Eval. Loss (mse): 0.000 <= 0.012 <= 542.001\n",
      "[AGT] [INFO    ] [16:06:26] =================== Finished Privacy Certified Training ===================\n",
      " 29%|██▊       | 2/7 [4:06:18<10:15:59, 7391.98s/it][AGT] [INFO    ] [16:06:26] =================== Starting Privacy Certified Training ===================\n",
      "[AGT] [INFO    ] [16:06:26] Starting epoch 1\n",
      "[AGT] [INFO    ] [16:06:39] Batch 1. Loss (mse): 0.340 <= 0.340 <= 0.340\n",
      "[AGT] [INFO    ] [16:08:49] Starting epoch 2\n",
      "[AGT] [INFO    ] [16:09:03] Batch 2. Loss (mse): 2.221 <= 2.222 <= 2.222\n",
      "[AGT] [INFO    ] [16:11:19] Starting epoch 3\n",
      "[AGT] [INFO    ] [16:11:33] Batch 3. Loss (mse): 0.249 <= 0.249 <= 0.249\n",
      "[AGT] [INFO    ] [16:13:49] Starting epoch 4\n",
      "[AGT] [INFO    ] [16:14:02] Batch 4. Loss (mse): 0.053 <= 0.053 <= 0.053\n",
      "[AGT] [INFO    ] [16:16:23] Starting epoch 5\n",
      "[AGT] [INFO    ] [16:16:37] Batch 5. Loss (mse): 0.075 <= 0.076 <= 0.076\n",
      "[AGT] [INFO    ] [16:18:56] Starting epoch 6\n",
      "[AGT] [INFO    ] [16:19:10] Batch 6. Loss (mse): 0.079 <= 0.080 <= 0.082\n",
      "[AGT] [INFO    ] [16:21:21] Starting epoch 7\n",
      "[AGT] [INFO    ] [16:21:35] Batch 7. Loss (mse): 0.060 <= 0.062 <= 0.064\n",
      "[AGT] [INFO    ] [16:23:51] Starting epoch 8\n",
      "[AGT] [INFO    ] [16:24:04] Batch 8. Loss (mse): 0.042 <= 0.045 <= 0.047\n",
      "[AGT] [INFO    ] [16:26:11] Starting epoch 9\n",
      "[AGT] [INFO    ] [16:26:25] Batch 9. Loss (mse): 0.027 <= 0.030 <= 0.032\n",
      "[AGT] [INFO    ] [16:28:30] Starting epoch 10\n",
      "[AGT] [INFO    ] [16:28:42] Batch 10. Loss (mse): 0.020 <= 0.024 <= 0.027\n",
      "[AGT] [INFO    ] [16:30:48] Starting epoch 11\n",
      "[AGT] [INFO    ] [16:31:02] Batch 11. Loss (mse): 0.021 <= 0.026 <= 0.031\n",
      "[AGT] [INFO    ] [16:33:17] Starting epoch 12\n",
      "[AGT] [INFO    ] [16:33:29] Batch 12. Loss (mse): 0.015 <= 0.020 <= 0.027\n",
      "[AGT] [INFO    ] [16:35:36] Starting epoch 13\n",
      "[AGT] [INFO    ] [16:35:49] Batch 13. Loss (mse): 0.011 <= 0.017 <= 0.025\n",
      "[AGT] [INFO    ] [16:38:02] Starting epoch 14\n",
      "[AGT] [INFO    ] [16:38:14] Batch 14. Loss (mse): 0.015 <= 0.024 <= 0.038\n",
      "[AGT] [INFO    ] [16:40:27] Starting epoch 15\n",
      "[AGT] [INFO    ] [16:40:42] Batch 15. Loss (mse): 0.008 <= 0.017 <= 0.032\n",
      "[AGT] [INFO    ] [16:42:54] Starting epoch 16\n",
      "[AGT] [INFO    ] [16:43:08] Batch 16. Loss (mse): 0.008 <= 0.021 <= 0.045\n",
      "[AGT] [INFO    ] [16:45:16] Starting epoch 17\n",
      "[AGT] [INFO    ] [16:45:28] Batch 17. Loss (mse): 0.005 <= 0.017 <= 0.044\n",
      "[AGT] [INFO    ] [16:47:38] Starting epoch 18\n",
      "[AGT] [INFO    ] [16:47:52] Batch 18. Loss (mse): 0.002 <= 0.015 <= 0.052\n",
      "[AGT] [INFO    ] [16:49:57] Starting epoch 19\n",
      "[AGT] [INFO    ] [16:50:09] Batch 19. Loss (mse): 0.001 <= 0.014 <= 0.067\n",
      "[AGT] [INFO    ] [16:52:16] Starting epoch 20\n",
      "[AGT] [INFO    ] [16:52:29] Batch 20. Loss (mse): 0.001 <= 0.016 <= 0.095\n",
      "[AGT] [INFO    ] [16:54:40] Starting epoch 21\n",
      "[AGT] [INFO    ] [16:54:53] Batch 21. Loss (mse): 0.000 <= 0.012 <= 0.108\n",
      "[AGT] [INFO    ] [16:57:02] Starting epoch 22\n",
      "[AGT] [INFO    ] [16:57:15] Batch 22. Loss (mse): 0.000 <= 0.011 <= 0.138\n",
      "[AGT] [INFO    ] [16:59:22] Starting epoch 23\n",
      "[AGT] [INFO    ] [16:59:36] Batch 23. Loss (mse): 0.000 <= 0.013 <= 0.210\n",
      "[AGT] [INFO    ] [17:01:57] Starting epoch 24\n",
      "[AGT] [INFO    ] [17:02:11] Batch 24. Loss (mse): 0.000 <= 0.012 <= 0.289\n",
      "[AGT] [INFO    ] [17:04:27] Starting epoch 25\n",
      "[AGT] [INFO    ] [17:04:41] Batch 25. Loss (mse): 0.000 <= 0.013 <= 0.393\n",
      "[AGT] [INFO    ] [17:06:58] Starting epoch 26\n",
      "[AGT] [INFO    ] [17:07:12] Batch 26. Loss (mse): 0.000 <= 0.011 <= 0.542\n",
      "[AGT] [INFO    ] [17:09:22] Starting epoch 27\n",
      "[AGT] [INFO    ] [17:09:36] Batch 27. Loss (mse): 0.000 <= 0.012 <= 0.796\n",
      "[AGT] [INFO    ] [17:11:52] Starting epoch 28\n",
      "[AGT] [INFO    ] [17:12:05] Batch 28. Loss (mse): 0.000 <= 0.011 <= 1.069\n",
      "[AGT] [INFO    ] [17:14:16] Starting epoch 29\n",
      "[AGT] [INFO    ] [17:14:30] Batch 29. Loss (mse): 0.000 <= 0.012 <= 1.540\n",
      "[AGT] [INFO    ] [17:16:46] Starting epoch 30\n",
      "[AGT] [INFO    ] [17:17:00] Batch 30. Loss (mse): 0.000 <= 0.011 <= 2.153\n",
      "[AGT] [INFO    ] [17:19:11] Starting epoch 31\n",
      "[AGT] [INFO    ] [17:19:25] Batch 31. Loss (mse): 0.000 <= 0.010 <= 2.977\n",
      "[AGT] [INFO    ] [17:21:36] Starting epoch 32\n",
      "[AGT] [INFO    ] [17:21:49] Batch 32. Loss (mse): 0.000 <= 0.012 <= 4.601\n",
      "[AGT] [INFO    ] [17:24:11] Starting epoch 33\n",
      "[AGT] [INFO    ] [17:24:25] Batch 33. Loss (mse): 0.000 <= 0.012 <= 6.559\n",
      "[AGT] [INFO    ] [17:26:39] Starting epoch 34\n",
      "[AGT] [INFO    ] [17:26:53] Batch 34. Loss (mse): 0.000 <= 0.011 <= 9.648\n",
      "[AGT] [INFO    ] [17:29:09] Starting epoch 35\n",
      "[AGT] [INFO    ] [17:29:23] Batch 35. Loss (mse): 0.000 <= 0.012 <= 14.509\n",
      "[AGT] [INFO    ] [17:31:36] Starting epoch 36\n",
      "[AGT] [INFO    ] [17:31:51] Batch 36. Loss (mse): 0.000 <= 0.011 <= 23.290\n",
      "[AGT] [INFO    ] [17:34:03] Starting epoch 37\n",
      "[AGT] [INFO    ] [17:34:16] Batch 37. Loss (mse): 0.000 <= 0.010 <= 34.993\n",
      "[AGT] [INFO    ] [17:36:33] Starting epoch 38\n",
      "[AGT] [INFO    ] [17:36:46] Batch 38. Loss (mse): 0.000 <= 0.010 <= 54.695\n",
      "[AGT] [INFO    ] [17:39:00] Starting epoch 39\n",
      "[AGT] [INFO    ] [17:39:13] Batch 39. Loss (mse): 0.000 <= 0.011 <= 84.846\n",
      "[AGT] [INFO    ] [17:41:31] Starting epoch 40\n",
      "[AGT] [INFO    ] [17:41:45] Batch 40. Loss (mse): 0.000 <= 0.010 <= 130.748\n",
      "[AGT] [INFO    ] [17:43:57] Starting epoch 41\n",
      "[AGT] [INFO    ] [17:44:11] Batch 41. Loss (mse): 0.000 <= 0.010 <= 199.026\n",
      "[AGT] [INFO    ] [17:46:25] Starting epoch 42\n",
      "[AGT] [INFO    ] [17:46:40] Batch 42. Loss (mse): 0.000 <= 0.011 <= 307.570\n",
      "[AGT] [INFO    ] [17:48:56] Starting epoch 43\n",
      "[AGT] [INFO    ] [17:49:09] Batch 43. Loss (mse): 0.000 <= 0.011 <= 444.585\n",
      "[AGT] [INFO    ] [17:51:31] Starting epoch 44\n",
      "[AGT] [INFO    ] [17:51:45] Batch 44. Loss (mse): 0.000 <= 0.010 <= 648.262\n",
      "[AGT] [INFO    ] [17:54:09] Starting epoch 45\n",
      "[AGT] [INFO    ] [17:54:24] Batch 45. Loss (mse): 0.000 <= 0.011 <= 909.946\n",
      "[AGT] [INFO    ] [17:56:53] Starting epoch 46\n",
      "[AGT] [INFO    ] [17:57:07] Batch 46. Loss (mse): 0.000 <= 0.010 <= 1284.024\n",
      "[AGT] [INFO    ] [17:59:29] Starting epoch 47\n",
      "[AGT] [INFO    ] [17:59:43] Batch 47. Loss (mse): 0.000 <= 0.011 <= 1800.271\n",
      "[AGT] [INFO    ] [18:02:09] Starting epoch 48\n",
      "[AGT] [INFO    ] [18:02:23] Batch 48. Loss (mse): 0.000 <= 0.011 <= 2315.412\n",
      "[AGT] [INFO    ] [18:04:52] Starting epoch 49\n",
      "[AGT] [INFO    ] [18:05:06] Batch 49. Loss (mse): 0.000 <= 0.011 <= 2998.847\n",
      "[AGT] [INFO    ] [18:07:23] Starting epoch 50\n",
      "[AGT] [INFO    ] [18:07:37] Batch 50. Loss (mse): 0.000 <= 0.010 <= 4002.069\n",
      "[AGT] [INFO    ] [18:09:46] Final Eval. Loss (mse): 0.000 <= 0.012 <= 5132.943\n",
      "[AGT] [INFO    ] [18:09:46] =================== Finished Privacy Certified Training ===================\n",
      " 43%|████▎     | 3/7 [6:09:39<8:13:04, 7396.08s/it] [AGT] [INFO    ] [18:09:47] =================== Starting Privacy Certified Training ===================\n",
      "[AGT] [INFO    ] [18:09:47] Starting epoch 1\n",
      "[AGT] [INFO    ] [18:10:01] Batch 1. Loss (mse): 0.340 <= 0.340 <= 0.340\n",
      "[AGT] [INFO    ] [18:12:20] Starting epoch 2\n",
      "[AGT] [INFO    ] [18:12:34] Batch 2. Loss (mse): 2.221 <= 2.222 <= 2.223\n",
      "[AGT] [INFO    ] [18:14:51] Starting epoch 3\n",
      "[AGT] [INFO    ] [18:15:05] Batch 3. Loss (mse): 0.248 <= 0.249 <= 0.250\n",
      "[AGT] [INFO    ] [18:17:21] Starting epoch 4\n",
      "[AGT] [INFO    ] [18:17:35] Batch 4. Loss (mse): 0.052 <= 0.053 <= 0.054\n",
      "[AGT] [INFO    ] [18:19:48] Starting epoch 5\n",
      "[AGT] [INFO    ] [18:20:01] Batch 5. Loss (mse): 0.074 <= 0.076 <= 0.077\n",
      "[AGT] [INFO    ] [18:22:16] Starting epoch 6\n",
      "[AGT] [INFO    ] [18:22:30] Batch 6. Loss (mse): 0.078 <= 0.080 <= 0.083\n",
      "[AGT] [INFO    ] [18:24:40] Starting epoch 7\n",
      "[AGT] [INFO    ] [18:24:53] Batch 7. Loss (mse): 0.059 <= 0.062 <= 0.065\n",
      "[AGT] [INFO    ] [18:26:59] Starting epoch 8\n",
      "[AGT] [INFO    ] [18:27:10] Batch 8. Loss (mse): 0.040 <= 0.045 <= 0.049\n",
      "[AGT] [INFO    ] [18:29:08] Starting epoch 9\n",
      "[AGT] [INFO    ] [18:29:18] Batch 9. Loss (mse): 0.025 <= 0.030 <= 0.035\n",
      "[AGT] [INFO    ] [18:31:23] Starting epoch 10\n",
      "[AGT] [INFO    ] [18:31:38] Batch 10. Loss (mse): 0.018 <= 0.024 <= 0.031\n",
      "[AGT] [INFO    ] [18:33:49] Starting epoch 11\n",
      "[AGT] [INFO    ] [18:34:00] Batch 11. Loss (mse): 0.017 <= 0.026 <= 0.037\n",
      "[AGT] [INFO    ] [18:36:09] Starting epoch 12\n",
      "[AGT] [INFO    ] [18:36:22] Batch 12. Loss (mse): 0.011 <= 0.020 <= 0.034\n",
      "[AGT] [INFO    ] [18:38:36] Starting epoch 13\n",
      "[AGT] [INFO    ] [18:38:46] Batch 13. Loss (mse): 0.007 <= 0.017 <= 0.036\n",
      "[AGT] [INFO    ] [18:40:40] Starting epoch 14\n",
      "[AGT] [INFO    ] [18:40:49] Batch 14. Loss (mse): 0.009 <= 0.024 <= 0.055\n",
      "[AGT] [INFO    ] [18:42:18] Starting epoch 15\n",
      "[AGT] [INFO    ] [18:42:26] Batch 15. Loss (mse): 0.003 <= 0.017 <= 0.053\n",
      "[AGT] [INFO    ] [18:43:53] Starting epoch 16\n",
      "[AGT] [INFO    ] [18:44:01] Batch 16. Loss (mse): 0.002 <= 0.021 <= 0.082\n",
      "[AGT] [INFO    ] [18:45:34] Starting epoch 17\n",
      "[AGT] [INFO    ] [18:45:43] Batch 17. Loss (mse): 0.001 <= 0.017 <= 0.092\n",
      "[AGT] [INFO    ] [18:47:24] Starting epoch 18\n",
      "[AGT] [INFO    ] [18:47:35] Batch 18. Loss (mse): 0.000 <= 0.015 <= 0.123\n",
      "[AGT] [INFO    ] [18:49:24] Starting epoch 19\n",
      "[AGT] [INFO    ] [18:49:34] Batch 19. Loss (mse): 0.000 <= 0.014 <= 0.174\n",
      "[AGT] [INFO    ] [18:51:20] Starting epoch 20\n",
      "[AGT] [INFO    ] [18:51:32] Batch 20. Loss (mse): 0.000 <= 0.016 <= 0.269\n",
      "[AGT] [INFO    ] [18:53:23] Starting epoch 21\n",
      "[AGT] [INFO    ] [18:53:33] Batch 21. Loss (mse): 0.000 <= 0.012 <= 0.334\n",
      "[AGT] [INFO    ] [18:55:23] Starting epoch 22\n",
      "[AGT] [INFO    ] [18:55:35] Batch 22. Loss (mse): 0.000 <= 0.011 <= 0.463\n",
      "[AGT] [INFO    ] [18:57:22] Starting epoch 23\n",
      "[AGT] [INFO    ] [18:57:32] Batch 23. Loss (mse): 0.000 <= 0.013 <= 0.731\n",
      "[AGT] [INFO    ] [18:59:24] Starting epoch 24\n",
      "[AGT] [INFO    ] [18:59:33] Batch 24. Loss (mse): 0.000 <= 0.012 <= 1.067\n",
      "[AGT] [INFO    ] [19:01:17] Starting epoch 25\n",
      "[AGT] [INFO    ] [19:01:30] Batch 25. Loss (mse): 0.000 <= 0.013 <= 1.493\n",
      "[AGT] [INFO    ] [19:03:12] Starting epoch 26\n",
      "[AGT] [INFO    ] [19:03:21] Batch 26. Loss (mse): 0.000 <= 0.011 <= 2.180\n",
      "[AGT] [INFO    ] [19:04:58] Starting epoch 27\n",
      "[AGT] [INFO    ] [19:05:09] Batch 27. Loss (mse): 0.000 <= 0.012 <= 3.328\n",
      "[AGT] [INFO    ] [19:06:56] Starting epoch 28\n",
      "[AGT] [INFO    ] [19:07:08] Batch 28. Loss (mse): 0.000 <= 0.011 <= 4.801\n",
      "[AGT] [INFO    ] [19:08:56] Starting epoch 29\n",
      "[AGT] [INFO    ] [19:09:07] Batch 29. Loss (mse): 0.000 <= 0.012 <= 7.469\n",
      "[AGT] [INFO    ] [19:10:41] Starting epoch 30\n",
      "[AGT] [INFO    ] [19:10:50] Batch 30. Loss (mse): 0.000 <= 0.011 <= 11.551\n",
      "[AGT] [INFO    ] [19:12:20] Starting epoch 31\n",
      "[AGT] [INFO    ] [19:12:29] Batch 31. Loss (mse): 0.000 <= 0.010 <= 18.008\n",
      "[AGT] [INFO    ] [19:14:02] Starting epoch 32\n",
      "[AGT] [INFO    ] [19:14:10] Batch 32. Loss (mse): 0.000 <= 0.012 <= 30.773\n",
      "[AGT] [INFO    ] [19:15:41] Starting epoch 33\n",
      "[AGT] [INFO    ] [19:15:49] Batch 33. Loss (mse): 0.000 <= 0.012 <= 49.674\n",
      "[AGT] [INFO    ] [19:17:13] Starting epoch 34\n",
      "[AGT] [INFO    ] [19:17:23] Batch 34. Loss (mse): 0.000 <= 0.011 <= 80.717\n",
      "[AGT] [INFO    ] [19:18:56] Starting epoch 35\n",
      "[AGT] [INFO    ] [19:19:05] Batch 35. Loss (mse): 0.000 <= 0.012 <= 127.982\n",
      "[AGT] [INFO    ] [19:20:32] Starting epoch 36\n",
      "[AGT] [INFO    ] [19:20:41] Batch 36. Loss (mse): 0.000 <= 0.011 <= 210.551\n",
      "[AGT] [INFO    ] [19:22:09] Starting epoch 37\n",
      "[AGT] [INFO    ] [19:22:18] Batch 37. Loss (mse): 0.000 <= 0.010 <= 318.431\n",
      "[AGT] [INFO    ] [19:23:41] Starting epoch 38\n",
      "[AGT] [INFO    ] [19:23:50] Batch 38. Loss (mse): 0.000 <= 0.010 <= 482.115\n",
      "[AGT] [INFO    ] [19:25:17] Starting epoch 39\n",
      "[AGT] [INFO    ] [19:25:26] Batch 39. Loss (mse): 0.000 <= 0.011 <= 709.972\n",
      "[AGT] [INFO    ] [19:26:56] Starting epoch 40\n",
      "[AGT] [INFO    ] [19:27:05] Batch 40. Loss (mse): 0.000 <= 0.010 <= 1022.631\n",
      "[AGT] [INFO    ] [19:28:37] Starting epoch 41\n",
      "[AGT] [INFO    ] [19:28:46] Batch 41. Loss (mse): 0.000 <= 0.010 <= 1440.629\n",
      "[AGT] [INFO    ] [19:30:14] Starting epoch 42\n",
      "[AGT] [INFO    ] [19:30:23] Batch 42. Loss (mse): 0.000 <= 0.011 <= 2060.321\n",
      "[AGT] [INFO    ] [19:31:47] Starting epoch 43\n",
      "[AGT] [INFO    ] [19:31:57] Batch 43. Loss (mse): 0.000 <= 0.011 <= 2763.752\n",
      "[AGT] [INFO    ] [19:33:24] Starting epoch 44\n",
      "[AGT] [INFO    ] [19:33:33] Batch 44. Loss (mse): 0.000 <= 0.010 <= 3739.792\n",
      "[AGT] [INFO    ] [19:35:07] Starting epoch 45\n",
      "[AGT] [INFO    ] [19:35:16] Batch 45. Loss (mse): 0.000 <= 0.011 <= 4875.043\n",
      "[AGT] [INFO    ] [19:36:48] Starting epoch 46\n",
      "[AGT] [INFO    ] [19:36:57] Batch 46. Loss (mse): 0.000 <= 0.010 <= 6409.590\n",
      "[AGT] [INFO    ] [19:38:32] Starting epoch 47\n",
      "[AGT] [INFO    ] [19:38:41] Batch 47. Loss (mse): 0.000 <= 0.011 <= 8430.690\n",
      "[AGT] [INFO    ] [19:40:12] Starting epoch 48\n",
      "[AGT] [INFO    ] [19:40:21] Batch 48. Loss (mse): 0.000 <= 0.011 <= 10119.478\n",
      "[AGT] [INFO    ] [19:41:52] Starting epoch 49\n",
      "[AGT] [INFO    ] [19:42:01] Batch 49. Loss (mse): 0.000 <= 0.011 <= 12368.402\n",
      "[AGT] [INFO    ] [19:43:33] Starting epoch 50\n",
      "[AGT] [INFO    ] [19:43:44] Batch 50. Loss (mse): 0.000 <= 0.010 <= 15587.350\n",
      "[AGT] [INFO    ] [19:45:19] Final Eval. Loss (mse): 0.000 <= 0.012 <= 18956.887\n",
      "[AGT] [INFO    ] [19:45:19] =================== Finished Privacy Certified Training ===================\n",
      " 57%|█████▋    | 4/7 [7:45:12<5:36:58, 6739.52s/it][AGT] [INFO    ] [19:45:20] =================== Starting Privacy Certified Training ===================\n",
      "[AGT] [INFO    ] [19:45:20] Starting epoch 1\n",
      "[AGT] [INFO    ] [19:45:28] Batch 1. Loss (mse): 0.340 <= 0.340 <= 0.340\n",
      "[AGT] [INFO    ] [19:47:01] Starting epoch 2\n",
      "[AGT] [INFO    ] [19:47:10] Batch 2. Loss (mse): 2.220 <= 2.222 <= 2.224\n",
      "[AGT] [INFO    ] [19:48:43] Starting epoch 3\n",
      "[AGT] [INFO    ] [19:48:51] Batch 3. Loss (mse): 0.248 <= 0.249 <= 0.250\n",
      "[AGT] [INFO    ] [19:50:22] Starting epoch 4\n",
      "[AGT] [INFO    ] [19:50:30] Batch 4. Loss (mse): 0.052 <= 0.053 <= 0.054\n",
      "[AGT] [INFO    ] [19:52:04] Starting epoch 5\n",
      "[AGT] [INFO    ] [19:52:12] Batch 5. Loss (mse): 0.073 <= 0.076 <= 0.078\n",
      "[AGT] [INFO    ] [19:53:45] Starting epoch 6\n",
      "[AGT] [INFO    ] [19:53:55] Batch 6. Loss (mse): 0.076 <= 0.080 <= 0.085\n",
      "[AGT] [INFO    ] [19:55:27] Starting epoch 7\n",
      "[AGT] [INFO    ] [19:55:36] Batch 7. Loss (mse): 0.055 <= 0.062 <= 0.069\n",
      "[AGT] [INFO    ] [19:57:10] Starting epoch 8\n",
      "[AGT] [INFO    ] [19:57:18] Batch 8. Loss (mse): 0.036 <= 0.045 <= 0.054\n",
      "[AGT] [INFO    ] [19:58:51] Starting epoch 9\n",
      "[AGT] [INFO    ] [19:59:00] Batch 9. Loss (mse): 0.021 <= 0.030 <= 0.041\n",
      "[AGT] [INFO    ] [20:00:31] Starting epoch 10\n",
      "[AGT] [INFO    ] [20:00:40] Batch 10. Loss (mse): 0.013 <= 0.024 <= 0.040\n",
      "[AGT] [INFO    ] [20:02:07] Starting epoch 11\n",
      "[AGT] [INFO    ] [20:02:16] Batch 11. Loss (mse): 0.011 <= 0.026 <= 0.050\n",
      "[AGT] [INFO    ] [20:03:46] Starting epoch 12\n",
      "[AGT] [INFO    ] [20:03:56] Batch 12. Loss (mse): 0.006 <= 0.020 <= 0.054\n",
      "[AGT] [INFO    ] [20:05:25] Starting epoch 13\n",
      "[AGT] [INFO    ] [20:05:34] Batch 13. Loss (mse): 0.003 <= 0.017 <= 0.064\n",
      "[AGT] [INFO    ] [20:07:08] Starting epoch 14\n",
      "[AGT] [INFO    ] [20:07:17] Batch 14. Loss (mse): 0.002 <= 0.024 <= 0.106\n",
      "[AGT] [INFO    ] [20:08:52] Starting epoch 15\n",
      "[AGT] [INFO    ] [20:09:01] Batch 15. Loss (mse): 0.000 <= 0.017 <= 0.120\n",
      "[AGT] [INFO    ] [20:10:32] Starting epoch 16\n",
      "[AGT] [INFO    ] [20:10:41] Batch 16. Loss (mse): 0.000 <= 0.021 <= 0.204\n",
      "[AGT] [INFO    ] [20:12:17] Starting epoch 17\n",
      "[AGT] [INFO    ] [20:12:26] Batch 17. Loss (mse): 0.000 <= 0.017 <= 0.256\n",
      "[AGT] [INFO    ] [20:14:01] Starting epoch 18\n",
      "[AGT] [INFO    ] [20:14:10] Batch 18. Loss (mse): 0.000 <= 0.015 <= 0.384\n",
      "[AGT] [INFO    ] [20:15:40] Starting epoch 19\n",
      "[AGT] [INFO    ] [20:15:49] Batch 19. Loss (mse): 0.000 <= 0.014 <= 0.592\n",
      "[AGT] [INFO    ] [20:17:21] Starting epoch 20\n",
      "[AGT] [INFO    ] [20:17:31] Batch 20. Loss (mse): 0.000 <= 0.016 <= 0.962\n",
      "[AGT] [INFO    ] [20:19:06] Starting epoch 21\n",
      "[AGT] [INFO    ] [20:19:16] Batch 21. Loss (mse): 0.000 <= 0.012 <= 1.282\n",
      "[AGT] [INFO    ] [20:20:47] Starting epoch 22\n",
      "[AGT] [INFO    ] [20:20:56] Batch 22. Loss (mse): 0.000 <= 0.011 <= 1.903\n",
      "[AGT] [INFO    ] [20:22:31] Starting epoch 23\n",
      "[AGT] [INFO    ] [20:22:40] Batch 23. Loss (mse): 0.000 <= 0.013 <= 3.103\n",
      "[AGT] [INFO    ] [20:24:12] Starting epoch 24\n",
      "[AGT] [INFO    ] [20:24:21] Batch 24. Loss (mse): 0.000 <= 0.012 <= 4.873\n",
      "[AGT] [INFO    ] [20:25:56] Starting epoch 25\n",
      "[AGT] [INFO    ] [20:26:04] Batch 25. Loss (mse): 0.000 <= 0.013 <= 7.400\n",
      "[AGT] [INFO    ] [20:27:37] Starting epoch 26\n",
      "[AGT] [INFO    ] [20:27:45] Batch 26. Loss (mse): 0.000 <= 0.011 <= 12.235\n",
      "[AGT] [INFO    ] [20:29:10] Starting epoch 27\n",
      "[AGT] [INFO    ] [20:29:19] Batch 27. Loss (mse): 0.000 <= 0.012 <= 21.028\n",
      "[AGT] [INFO    ] [20:30:53] Starting epoch 28\n",
      "[AGT] [INFO    ] [20:31:02] Batch 28. Loss (mse): 0.000 <= 0.011 <= 35.206\n",
      "[AGT] [INFO    ] [20:32:29] Starting epoch 29\n",
      "[AGT] [INFO    ] [20:32:38] Batch 29. Loss (mse): 0.000 <= 0.012 <= 61.389\n",
      "[AGT] [INFO    ] [20:34:14] Starting epoch 30\n",
      "[AGT] [INFO    ] [20:34:23] Batch 30. Loss (mse): 0.000 <= 0.011 <= 103.727\n",
      "[AGT] [INFO    ] [20:35:54] Starting epoch 31\n",
      "[AGT] [INFO    ] [20:36:03] Batch 31. Loss (mse): 0.000 <= 0.010 <= 169.528\n",
      "[AGT] [INFO    ] [20:37:36] Starting epoch 32\n",
      "[AGT] [INFO    ] [20:37:46] Batch 32. Loss (mse): 0.000 <= 0.012 <= 288.856\n",
      "[AGT] [INFO    ] [20:39:22] Starting epoch 33\n",
      "[AGT] [INFO    ] [20:39:31] Batch 33. Loss (mse): 0.000 <= 0.012 <= 457.894\n",
      "[AGT] [INFO    ] [20:41:05] Starting epoch 34\n",
      "[AGT] [INFO    ] [20:41:14] Batch 34. Loss (mse): 0.000 <= 0.011 <= 702.343\n",
      "[AGT] [INFO    ] [20:42:51] Starting epoch 35\n",
      "[AGT] [INFO    ] [20:43:01] Batch 35. Loss (mse): 0.000 <= 0.012 <= 1039.704\n",
      "[AGT] [INFO    ] [20:44:35] Starting epoch 36\n",
      "[AGT] [INFO    ] [20:44:45] Batch 36. Loss (mse): 0.000 <= 0.011 <= 1590.464\n",
      "[AGT] [INFO    ] [20:46:17] Starting epoch 37\n",
      "[AGT] [INFO    ] [20:46:26] Batch 37. Loss (mse): 0.000 <= 0.010 <= 2197.286\n",
      "[AGT] [INFO    ] [20:47:59] Starting epoch 38\n",
      "[AGT] [INFO    ] [20:48:08] Batch 38. Loss (mse): 0.000 <= 0.010 <= 3063.970\n",
      "[AGT] [INFO    ] [20:49:38] Starting epoch 39\n",
      "[AGT] [INFO    ] [20:49:47] Batch 39. Loss (mse): 0.000 <= 0.011 <= 4148.445\n",
      "[AGT] [INFO    ] [20:51:14] Starting epoch 40\n",
      "[AGT] [INFO    ] [20:51:23] Batch 40. Loss (mse): 0.000 <= 0.010 <= 5514.175\n",
      "[AGT] [INFO    ] [20:52:53] Starting epoch 41\n",
      "[AGT] [INFO    ] [20:53:02] Batch 41. Loss (mse): 0.000 <= 0.010 <= 7182.590\n",
      "[AGT] [INFO    ] [20:54:32] Starting epoch 42\n",
      "[AGT] [INFO    ] [20:54:40] Batch 42. Loss (mse): 0.000 <= 0.011 <= 9561.836\n",
      "[AGT] [INFO    ] [20:56:14] Starting epoch 43\n",
      "[AGT] [INFO    ] [20:56:23] Batch 43. Loss (mse): 0.000 <= 0.011 <= 12000.647\n",
      "[AGT] [INFO    ] [20:57:53] Starting epoch 44\n",
      "[AGT] [INFO    ] [20:58:02] Batch 44. Loss (mse): 0.000 <= 0.010 <= 15247.553\n",
      "[AGT] [INFO    ] [20:59:30] Starting epoch 45\n",
      "[AGT] [INFO    ] [20:59:39] Batch 45. Loss (mse): 0.000 <= 0.011 <= 18728.740\n",
      "[AGT] [INFO    ] [21:01:03] Starting epoch 46\n",
      "[AGT] [INFO    ] [21:01:13] Batch 46. Loss (mse): 0.000 <= 0.010 <= 23298.252\n",
      "[AGT] [INFO    ] [21:02:44] Starting epoch 47\n",
      "[AGT] [INFO    ] [21:02:52] Batch 47. Loss (mse): 0.000 <= 0.011 <= 29198.168\n",
      "[AGT] [INFO    ] [21:04:29] Starting epoch 48\n",
      "[AGT] [INFO    ] [21:04:38] Batch 48. Loss (mse): 0.000 <= 0.011 <= 33295.496\n",
      "[AGT] [INFO    ] [21:06:06] Starting epoch 49\n",
      "[AGT] [INFO    ] [21:06:14] Batch 49. Loss (mse): 0.000 <= 0.011 <= 39001.930\n",
      "[AGT] [INFO    ] [21:07:54] Starting epoch 50\n",
      "[AGT] [INFO    ] [21:08:03] Batch 50. Loss (mse): 0.000 <= 0.010 <= 47159.484\n",
      "[AGT] [INFO    ] [21:09:46] Final Eval. Loss (mse): 0.000 <= 0.012 <= 55187.051\n",
      "[AGT] [INFO    ] [21:09:46] =================== Finished Privacy Certified Training ===================\n",
      " 71%|███████▏  | 5/7 [9:09:38<3:24:32, 6136.20s/it][AGT] [INFO    ] [21:09:46] =================== Starting Privacy Certified Training ===================\n",
      "[AGT] [INFO    ] [21:09:46] Starting epoch 1\n",
      "[AGT] [INFO    ] [21:09:55] Batch 1. Loss (mse): 0.340 <= 0.340 <= 0.340\n",
      "[AGT] [INFO    ] [21:11:39] Starting epoch 2\n",
      "[AGT] [INFO    ] [21:11:47] Batch 2. Loss (mse): 2.216 <= 2.222 <= 2.227\n",
      "[AGT] [INFO    ] [21:13:25] Starting epoch 3\n",
      "[AGT] [INFO    ] [21:13:36] Batch 3. Loss (mse): 0.245 <= 0.249 <= 0.252\n",
      "[AGT] [INFO    ] [21:15:11] Starting epoch 4\n",
      "[AGT] [INFO    ] [21:15:19] Batch 4. Loss (mse): 0.050 <= 0.053 <= 0.056\n",
      "[AGT] [INFO    ] [21:16:53] Starting epoch 5\n",
      "[AGT] [INFO    ] [21:17:03] Batch 5. Loss (mse): 0.069 <= 0.076 <= 0.082\n",
      "[AGT] [INFO    ] [21:18:45] Starting epoch 6\n",
      "[AGT] [INFO    ] [21:18:54] Batch 6. Loss (mse): 0.069 <= 0.080 <= 0.092\n",
      "[AGT] [INFO    ] [21:20:26] Starting epoch 7\n",
      "[AGT] [INFO    ] [21:20:35] Batch 7. Loss (mse): 0.047 <= 0.062 <= 0.080\n",
      "[AGT] [INFO    ] [21:22:01] Starting epoch 8\n",
      "[AGT] [INFO    ] [21:22:10] Batch 8. Loss (mse): 0.026 <= 0.045 <= 0.070\n",
      "[AGT] [INFO    ] [21:23:41] Starting epoch 9\n",
      "[AGT] [INFO    ] [21:23:50] Batch 9. Loss (mse): 0.011 <= 0.030 <= 0.063\n",
      "[AGT] [INFO    ] [21:25:23] Starting epoch 10\n",
      "[AGT] [INFO    ] [21:25:32] Batch 10. Loss (mse): 0.004 <= 0.024 <= 0.077\n",
      "[AGT] [INFO    ] [21:27:02] Starting epoch 11\n",
      "[AGT] [INFO    ] [21:27:11] Batch 11. Loss (mse): 0.002 <= 0.026 <= 0.111\n",
      "[AGT] [INFO    ] [21:28:36] Starting epoch 12\n",
      "[AGT] [INFO    ] [21:28:45] Batch 12. Loss (mse): 0.000 <= 0.020 <= 0.149\n",
      "[AGT] [INFO    ] [21:30:17] Starting epoch 13\n",
      "[AGT] [INFO    ] [21:30:26] Batch 13. Loss (mse): 0.000 <= 0.017 <= 0.215\n",
      "[AGT] [INFO    ] [21:32:02] Starting epoch 14\n",
      "[AGT] [INFO    ] [21:32:12] Batch 14. Loss (mse): 0.000 <= 0.024 <= 0.394\n",
      "[AGT] [INFO    ] [21:33:43] Starting epoch 15\n",
      "[AGT] [INFO    ] [21:33:52] Batch 15. Loss (mse): 0.000 <= 0.017 <= 0.548\n",
      "[AGT] [INFO    ] [21:35:26] Starting epoch 16\n",
      "[AGT] [INFO    ] [21:35:36] Batch 16. Loss (mse): 0.000 <= 0.021 <= 1.025\n",
      "[AGT] [INFO    ] [21:37:08] Starting epoch 17\n",
      "[AGT] [INFO    ] [21:37:18] Batch 17. Loss (mse): 0.000 <= 0.017 <= 1.456\n",
      "[AGT] [INFO    ] [21:38:46] Starting epoch 18\n",
      "[AGT] [INFO    ] [21:38:56] Batch 18. Loss (mse): 0.000 <= 0.015 <= 2.401\n",
      "[AGT] [INFO    ] [21:40:22] Starting epoch 19\n",
      "[AGT] [INFO    ] [21:40:31] Batch 19. Loss (mse): 0.000 <= 0.014 <= 4.094\n",
      "[AGT] [INFO    ] [21:41:59] Starting epoch 20\n",
      "[AGT] [INFO    ] [21:42:08] Batch 20. Loss (mse): 0.000 <= 0.016 <= 7.300\n",
      "[AGT] [INFO    ] [21:43:34] Starting epoch 21\n",
      "[AGT] [INFO    ] [21:43:43] Batch 21. Loss (mse): 0.000 <= 0.012 <= 11.588\n",
      "[AGT] [INFO    ] [21:45:13] Starting epoch 22\n",
      "[AGT] [INFO    ] [21:45:22] Batch 22. Loss (mse): 0.000 <= 0.011 <= 20.870\n",
      "[AGT] [INFO    ] [21:46:56] Starting epoch 23\n",
      "[AGT] [INFO    ] [21:47:05] Batch 23. Loss (mse): 0.000 <= 0.013 <= 39.561\n",
      "[AGT] [INFO    ] [21:48:40] Starting epoch 24\n",
      "[AGT] [INFO    ] [21:48:49] Batch 24. Loss (mse): 0.000 <= 0.012 <= 73.975\n",
      "[AGT] [INFO    ] [21:50:23] Starting epoch 25\n",
      "[AGT] [INFO    ] [21:50:32] Batch 25. Loss (mse): 0.000 <= 0.013 <= 129.759\n",
      "[AGT] [INFO    ] [21:52:04] Starting epoch 26\n",
      "[AGT] [INFO    ] [21:52:13] Batch 26. Loss (mse): 0.000 <= 0.011 <= 232.186\n",
      "[AGT] [INFO    ] [21:53:50] Starting epoch 27\n",
      "[AGT] [INFO    ] [21:53:58] Batch 27. Loss (mse): 0.000 <= 0.012 <= 403.400\n",
      "[AGT] [INFO    ] [21:55:32] Starting epoch 28\n",
      "[AGT] [INFO    ] [21:55:41] Batch 28. Loss (mse): 0.000 <= 0.011 <= 658.208\n",
      "[AGT] [INFO    ] [21:57:16] Starting epoch 29\n",
      "[AGT] [INFO    ] [21:57:25] Batch 29. Loss (mse): 0.000 <= 0.012 <= 1072.554\n",
      "[AGT] [INFO    ] [21:59:01] Starting epoch 30\n",
      "[AGT] [INFO    ] [21:59:10] Batch 30. Loss (mse): 0.000 <= 0.011 <= 1636.244\n",
      "[AGT] [INFO    ] [22:00:49] Starting epoch 31\n",
      "[AGT] [INFO    ] [22:00:57] Batch 31. Loss (mse): 0.000 <= 0.010 <= 2378.031\n",
      "[AGT] [INFO    ] [22:02:35] Starting epoch 32\n",
      "[AGT] [INFO    ] [22:02:44] Batch 32. Loss (mse): 0.000 <= 0.012 <= 3606.783\n",
      "[AGT] [INFO    ] [22:04:14] Starting epoch 33\n",
      "[AGT] [INFO    ] [22:04:23] Batch 33. Loss (mse): 0.000 <= 0.012 <= 5083.266\n",
      "[AGT] [INFO    ] [22:05:51] Starting epoch 34\n",
      "[AGT] [INFO    ] [22:06:00] Batch 34. Loss (mse): 0.000 <= 0.011 <= 6882.513\n",
      "[AGT] [INFO    ] [22:07:27] Starting epoch 35\n",
      "[AGT] [INFO    ] [22:07:36] Batch 35. Loss (mse): 0.000 <= 0.012 <= 9144.688\n",
      "[AGT] [INFO    ] [22:09:10] Starting epoch 36\n",
      "[AGT] [INFO    ] [22:09:20] Batch 36. Loss (mse): 0.000 <= 0.011 <= 12676.484\n",
      "[AGT] [INFO    ] [22:10:49] Starting epoch 37\n",
      "[AGT] [INFO    ] [22:10:58] Batch 37. Loss (mse): 0.000 <= 0.010 <= 15729.866\n",
      "[AGT] [INFO    ] [22:12:24] Starting epoch 38\n",
      "[AGT] [INFO    ] [22:12:33] Batch 38. Loss (mse): 0.000 <= 0.010 <= 20065.504\n",
      "[AGT] [INFO    ] [22:14:03] Starting epoch 39\n",
      "[AGT] [INFO    ] [22:14:12] Batch 39. Loss (mse): 0.000 <= 0.011 <= 24898.461\n",
      "[AGT] [INFO    ] [22:15:42] Starting epoch 40\n",
      "[AGT] [INFO    ] [22:15:51] Batch 40. Loss (mse): 0.000 <= 0.010 <= 30572.262\n",
      "[AGT] [INFO    ] [22:17:17] Starting epoch 41\n",
      "[AGT] [INFO    ] [22:17:26] Batch 41. Loss (mse): 0.000 <= 0.010 <= 36951.496\n",
      "[AGT] [INFO    ] [22:18:56] Starting epoch 42\n",
      "[AGT] [INFO    ] [22:19:05] Batch 42. Loss (mse): 0.000 <= 0.011 <= 46014.906\n",
      "[AGT] [INFO    ] [22:20:32] Starting epoch 43\n",
      "[AGT] [INFO    ] [22:20:41] Batch 43. Loss (mse): 0.000 <= 0.011 <= 54318.953\n",
      "[AGT] [INFO    ] [22:22:09] Starting epoch 44\n",
      "[AGT] [INFO    ] [22:22:18] Batch 44. Loss (mse): 0.000 <= 0.010 <= 65172.699\n",
      "[AGT] [INFO    ] [22:23:49] Starting epoch 45\n",
      "[AGT] [INFO    ] [22:23:58] Batch 45. Loss (mse): 0.000 <= 0.011 <= 75877.758\n",
      "[AGT] [INFO    ] [22:25:29] Starting epoch 46\n",
      "[AGT] [INFO    ] [22:25:38] Batch 46. Loss (mse): 0.000 <= 0.010 <= 89806.492\n",
      "[AGT] [INFO    ] [22:27:10] Starting epoch 47\n",
      "[AGT] [INFO    ] [22:27:19] Batch 47. Loss (mse): 0.000 <= 0.011 <= 107742.516\n",
      "[AGT] [INFO    ] [22:28:50] Starting epoch 48\n",
      "[AGT] [INFO    ] [22:28:59] Batch 48. Loss (mse): 0.000 <= 0.011 <= 117352.648\n",
      "[AGT] [INFO    ] [22:30:34] Starting epoch 49\n",
      "[AGT] [INFO    ] [22:30:44] Batch 49. Loss (mse): 0.000 <= 0.011 <= 132277.609\n",
      "[AGT] [INFO    ] [22:32:17] Starting epoch 50\n",
      "[AGT] [INFO    ] [22:32:26] Batch 50. Loss (mse): 0.000 <= 0.010 <= 154073.125\n",
      "[AGT] [INFO    ] [22:33:53] Final Eval. Loss (mse): 0.000 <= 0.012 <= 174088.344\n",
      "[AGT] [INFO    ] [22:33:53] =================== Finished Privacy Certified Training ===================\n",
      " 86%|████████▌ | 6/7 [10:33:45<1:36:05, 5765.84s/it][AGT] [INFO    ] [22:33:53] =================== Starting Privacy Certified Training ===================\n",
      "[AGT] [INFO    ] [22:33:53] Starting epoch 1\n",
      "[AGT] [INFO    ] [22:34:02] Batch 1. Loss (mse): 0.340 <= 0.340 <= 0.340\n",
      "[AGT] [INFO    ] [22:35:29] Starting epoch 2\n",
      "[AGT] [INFO    ] [22:35:38] Batch 2. Loss (mse): 2.211 <= 2.222 <= 2.232\n",
      "[AGT] [INFO    ] [22:37:04] Starting epoch 3\n",
      "[AGT] [INFO    ] [22:37:13] Batch 3. Loss (mse): 0.242 <= 0.249 <= 0.256\n",
      "[AGT] [INFO    ] [22:38:45] Starting epoch 4\n",
      "[AGT] [INFO    ] [22:38:55] Batch 4. Loss (mse): 0.047 <= 0.053 <= 0.059\n",
      "[AGT] [INFO    ] [22:40:25] Starting epoch 5\n",
      "[AGT] [INFO    ] [22:40:34] Batch 5. Loss (mse): 0.063 <= 0.076 <= 0.089\n",
      "[AGT] [INFO    ] [22:42:03] Starting epoch 6\n",
      "[AGT] [INFO    ] [22:42:12] Batch 6. Loss (mse): 0.060 <= 0.080 <= 0.106\n",
      "[AGT] [INFO    ] [22:43:41] Starting epoch 7\n",
      "[AGT] [INFO    ] [22:43:51] Batch 7. Loss (mse): 0.034 <= 0.062 <= 0.102\n",
      "[AGT] [INFO    ] [22:45:18] Starting epoch 8\n",
      "[AGT] [INFO    ] [22:45:27] Batch 8. Loss (mse): 0.014 <= 0.045 <= 0.104\n",
      "[AGT] [INFO    ] [22:46:56] Starting epoch 9\n",
      "[AGT] [INFO    ] [22:47:06] Batch 9. Loss (mse): 0.003 <= 0.030 <= 0.114\n",
      "[AGT] [INFO    ] [22:48:34] Starting epoch 10\n",
      "[AGT] [INFO    ] [22:48:43] Batch 10. Loss (mse): 0.000 <= 0.024 <= 0.172\n",
      "[AGT] [INFO    ] [22:50:03] Starting epoch 11\n",
      "[AGT] [INFO    ] [22:50:12] Batch 11. Loss (mse): 0.000 <= 0.026 <= 0.282\n",
      "[AGT] [INFO    ] [22:51:43] Starting epoch 12\n",
      "[AGT] [INFO    ] [22:51:53] Batch 12. Loss (mse): 0.000 <= 0.020 <= 0.448\n",
      "[AGT] [INFO    ] [22:53:20] Starting epoch 13\n",
      "[AGT] [INFO    ] [22:53:29] Batch 13. Loss (mse): 0.000 <= 0.017 <= 0.730\n",
      "[AGT] [INFO    ] [22:54:57] Starting epoch 14\n",
      "[AGT] [INFO    ] [22:55:07] Batch 14. Loss (mse): 0.000 <= 0.024 <= 1.404\n",
      "[AGT] [INFO    ] [22:56:33] Starting epoch 15\n",
      "[AGT] [INFO    ] [22:56:42] Batch 15. Loss (mse): 0.000 <= 0.017 <= 2.183\n",
      "[AGT] [INFO    ] [22:58:16] Starting epoch 16\n",
      "[AGT] [INFO    ] [22:58:25] Batch 16. Loss (mse): 0.000 <= 0.021 <= 4.339\n",
      "[AGT] [INFO    ] [22:59:54] Starting epoch 17\n",
      "[AGT] [INFO    ] [23:00:04] Batch 17. Loss (mse): 0.000 <= 0.017 <= 7.026\n",
      "[AGT] [INFO    ] [23:01:32] Starting epoch 18\n",
      "[AGT] [INFO    ] [23:01:41] Batch 18. Loss (mse): 0.000 <= 0.015 <= 13.350\n",
      "[AGT] [INFO    ] [23:03:12] Starting epoch 19\n",
      "[AGT] [INFO    ] [23:03:21] Batch 19. Loss (mse): 0.000 <= 0.014 <= 26.844\n",
      "[AGT] [INFO    ] [23:04:53] Starting epoch 20\n",
      "[AGT] [INFO    ] [23:05:02] Batch 20. Loss (mse): 0.000 <= 0.016 <= 54.950\n",
      "[AGT] [INFO    ] [23:06:24] Starting epoch 21\n",
      "[AGT] [INFO    ] [23:06:33] Batch 21. Loss (mse): 0.000 <= 0.012 <= 101.211\n",
      "[AGT] [INFO    ] [23:08:03] Starting epoch 22\n",
      "[AGT] [INFO    ] [23:08:12] Batch 22. Loss (mse): 0.000 <= 0.011 <= 192.228\n",
      "[AGT] [INFO    ] [23:09:43] Starting epoch 23\n",
      "[AGT] [INFO    ] [23:09:53] Batch 23. Loss (mse): 0.000 <= 0.013 <= 360.635\n",
      "[AGT] [INFO    ] [23:11:19] Starting epoch 24\n",
      "[AGT] [INFO    ] [23:11:28] Batch 24. Loss (mse): 0.000 <= 0.012 <= 643.827\n",
      "[AGT] [INFO    ] [23:12:58] Starting epoch 25\n",
      "[AGT] [INFO    ] [23:13:08] Batch 25. Loss (mse): 0.000 <= 0.013 <= 1047.567\n",
      "[AGT] [INFO    ] [23:14:43] Starting epoch 26\n",
      "[AGT] [INFO    ] [23:14:53] Batch 26. Loss (mse): 0.000 <= 0.011 <= 1696.458\n",
      "[AGT] [INFO    ] [23:16:18] Starting epoch 27\n",
      "[AGT] [INFO    ] [23:16:28] Batch 27. Loss (mse): 0.000 <= 0.012 <= 2672.072\n",
      "[AGT] [INFO    ] [23:17:57] Starting epoch 28\n",
      "[AGT] [INFO    ] [23:18:06] Batch 28. Loss (mse): 0.000 <= 0.011 <= 3930.214\n",
      "[AGT] [INFO    ] [23:19:38] Starting epoch 29\n",
      "[AGT] [INFO    ] [23:19:47] Batch 29. Loss (mse): 0.000 <= 0.012 <= 5840.146\n",
      "[AGT] [INFO    ] [23:21:21] Starting epoch 30\n",
      "[AGT] [INFO    ] [23:21:30] Batch 30. Loss (mse): 0.000 <= 0.011 <= 8122.575\n",
      "[AGT] [INFO    ] [23:23:04] Starting epoch 31\n",
      "[AGT] [INFO    ] [23:23:14] Batch 31. Loss (mse): 0.000 <= 0.010 <= 10820.606\n",
      "[AGT] [INFO    ] [23:24:47] Starting epoch 32\n",
      "[AGT] [INFO    ] [23:24:57] Batch 32. Loss (mse): 0.000 <= 0.012 <= 15214.049\n",
      "[AGT] [INFO    ] [23:26:32] Starting epoch 33\n",
      "[AGT] [INFO    ] [23:26:40] Batch 33. Loss (mse): 0.000 <= 0.012 <= 19937.637\n",
      "[AGT] [INFO    ] [23:28:15] Starting epoch 34\n",
      "[AGT] [INFO    ] [23:28:24] Batch 34. Loss (mse): 0.000 <= 0.011 <= 25169.457\n",
      "[AGT] [INFO    ] [23:30:04] Starting epoch 35\n",
      "[AGT] [INFO    ] [23:30:12] Batch 35. Loss (mse): 0.000 <= 0.012 <= 31523.996\n",
      "[AGT] [INFO    ] [23:31:48] Starting epoch 36\n",
      "[AGT] [INFO    ] [23:31:56] Batch 36. Loss (mse): 0.000 <= 0.011 <= 41447.988\n",
      "[AGT] [INFO    ] [23:33:33] Starting epoch 37\n",
      "[AGT] [INFO    ] [23:33:42] Batch 37. Loss (mse): 0.000 <= 0.010 <= 48695.273\n",
      "[AGT] [INFO    ] [23:35:25] Starting epoch 38\n",
      "[AGT] [INFO    ] [23:35:39] Batch 38. Loss (mse): 0.000 <= 0.010 <= 59357.988\n",
      "[AGT] [INFO    ] [23:37:15] Starting epoch 39\n",
      "[AGT] [INFO    ] [23:37:24] Batch 39. Loss (mse): 0.000 <= 0.011 <= 70494.531\n",
      "[AGT] [INFO    ] [23:38:57] Starting epoch 40\n",
      "[AGT] [INFO    ] [23:39:06] Batch 40. Loss (mse): 0.000 <= 0.010 <= 83184.914\n",
      "[AGT] [INFO    ] [23:40:39] Starting epoch 41\n",
      "[AGT] [INFO    ] [23:40:48] Batch 41. Loss (mse): 0.000 <= 0.010 <= 96844.609\n",
      "[AGT] [INFO    ] [23:42:17] Starting epoch 42\n",
      "[AGT] [INFO    ] [23:42:26] Batch 42. Loss (mse): 0.000 <= 0.011 <= 116613.867\n",
      "[AGT] [INFO    ] [23:43:57] Starting epoch 43\n",
      "[AGT] [INFO    ] [23:44:06] Batch 43. Loss (mse): 0.000 <= 0.011 <= 133454.828\n",
      "[AGT] [INFO    ] [23:45:41] Starting epoch 44\n",
      "[AGT] [INFO    ] [23:45:50] Batch 44. Loss (mse): 0.000 <= 0.010 <= 155534.891\n",
      "[AGT] [INFO    ] [23:47:22] Starting epoch 45\n",
      "[AGT] [INFO    ] [23:47:31] Batch 45. Loss (mse): 0.000 <= 0.011 <= 176216.766\n",
      "[AGT] [INFO    ] [23:49:01] Starting epoch 46\n",
      "[AGT] [INFO    ] [23:49:10] Batch 46. Loss (mse): 0.000 <= 0.010 <= 203324.547\n",
      "[AGT] [INFO    ] [23:50:35] Starting epoch 47\n",
      "[AGT] [INFO    ] [23:50:44] Batch 47. Loss (mse): 0.000 <= 0.011 <= 238506.516\n",
      "[AGT] [INFO    ] [23:52:14] Starting epoch 48\n",
      "[AGT] [INFO    ] [23:52:23] Batch 48. Loss (mse): 0.000 <= 0.011 <= 253731.938\n",
      "[AGT] [INFO    ] [23:53:55] Starting epoch 49\n",
      "[AGT] [INFO    ] [23:54:04] Batch 49. Loss (mse): 0.000 <= 0.011 <= 280340.750\n",
      "[AGT] [INFO    ] [23:55:33] Starting epoch 50\n",
      "[AGT] [INFO    ] [23:55:42] Batch 50. Loss (mse): 0.000 <= 0.010 <= 320233.219\n",
      "[AGT] [INFO    ] [23:57:11] Final Eval. Loss (mse): 0.000 <= 0.012 <= 355244.500\n",
      "[AGT] [INFO    ] [23:57:11] =================== Finished Privacy Certified Training ===================\n",
      "100%|██████████| 7/7 [11:57:03<00:00, 6146.27s/it]  \n"
     ]
    }
   ],
   "source": [
    "# to use privacy-safe certificates, we need to run AGT for a range of k_private values\n",
    "\n",
    "# we'll just pick a reasonable range of k_private values. adding more values will increase the runtime\n",
    "# but also result in tighter privacy results. even a few values are sufficient to demonstrate tighter privacy\n",
    "\n",
    "k_private_values = [1, 2, 5, 10, 20, 50, 100] \n",
    "privacy_bounded_models = {}\n",
    "config = copy.deepcopy(nominal_config)\n",
    "# config.log_level = \"WARNING\"\n",
    "\n",
    "for k_private in tqdm.tqdm(k_private_values):\n",
    "    # update config\n",
    "    config.k_private = k_private\n",
    "    # form bounded model\n",
    "    torch.manual_seed(1)\n",
    "    # get the nn model\n",
    "    model = torch.nn.Sequential(torch.nn.Linear(11, 64), torch.nn.ReLU(), torch.nn.Linear(64, 1)).to(config.device)\n",
    "    bounded_model = IntervalBoundedModel(model, trainable=True)\n",
    "    # dl_train = torch.utils.data.DataLoader(dataset_train, batch_size=batchsize, shuffle=True)\n",
    "    # run AGT\n",
    "    agt.privacy_certified_training(bounded_model, config, train_loader, test_loader, k=k_private)\n",
    "    privacy_bounded_models[k_private] = bounded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53ac7303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from abstract_gradient_training.bounded_models import BoundedModel\n",
    "def noisy_test_mse(\n",
    "    model: torch.nn.Sequential | BoundedModel,\n",
    "    batch: torch.Tensor,\n",
    "    labels: torch.Tensor,\n",
    "    *,\n",
    "    noise_level: float | torch.Tensor = 0.0,\n",
    "    noise_type: str = \"laplace\",\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Given a pytorch (or bounded) model, calculate the prediction accuracy on a batch of the test set when adding the\n",
    "    specified noise to the predictions.\n",
    "    NOTE: For now, this function only supports binary classification via the noise + threshold dp mechanism. This\n",
    "          should be extended to support multi-class problems via the noisy-argmax mechanism in the future.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Sequential | BoundedModel): The model to evaluate.\n",
    "        batch (torch.Tensor): Input batch of data (shape [batchsize, ...]).\n",
    "        labels (torch.Tensor): Targets for the input batch (shape [batchsize, ]).\n",
    "        noise_level (float | torch.Tensor, optional): Noise level for privacy-preserving predictions using the laplace\n",
    "            mechanism. Can either be a float or a torch.Tensor of shape (batchsize, ).\n",
    "        noise_type (str, optional): Type of noise to add to the predictions, one of [\"laplace\", \"cauchy\"].\n",
    "\n",
    "    Returns:\n",
    "        float: The noisy accuracy of the model on the test set.\n",
    "    \"\"\"\n",
    "    # get the test batch and send it to the correct device\n",
    "    if isinstance(model, BoundedModel):\n",
    "        device = torch.device(model.device) if model.device != -1 else torch.device(\"cpu\")\n",
    "    else:\n",
    "        device = torch.device(next(model.parameters()).device)\n",
    "    batch = batch.to(device)\n",
    "\n",
    "    # validate the labels\n",
    "    if labels.dim() > 1:\n",
    "        labels = labels.squeeze()\n",
    "    labels = labels.to(device).type(torch.int64)\n",
    "    assert labels.dim() == 1, \"Labels must be of shape (batchsize, )\"\n",
    "\n",
    "    # validate the noise parameters and set up the distribution\n",
    "    assert noise_type in [\"laplace\", \"cauchy\"], f\"Noise type must be one of ['laplace', 'cauchy'], got {noise_type}\"\n",
    "    noise_level += 1e-7  # can't set distributions scale to zero\n",
    "    noise_level = torch.tensor(noise_level) if isinstance(noise_level, float) else noise_level\n",
    "    noise_level = noise_level.to(device).type(batch.dtype)  # type: ignore\n",
    "    noise_level = noise_level.expand(labels.size())\n",
    "    if noise_type == \"laplace\":\n",
    "        noise_distribution = torch.distributions.Laplace(0, noise_level)\n",
    "    else:\n",
    "        noise_distribution = torch.distributions.Cauchy(0, noise_level)\n",
    "\n",
    "    # # nominal, lower and upper bounds for the forward pass\n",
    "    # logit_n = model.forward(batch).squeeze()\n",
    "\n",
    "    # # transform 2-logit models to a single output\n",
    "    # if logit_n.shape[-1] == 2:\n",
    "    #     logit_n = logit_n[:, 1] - logit_n[:, 0]\n",
    "    # if logit_n.dim() > 1:\n",
    "    #     raise NotImplementedError(\"Noisy accuracy is not supported for multi-class classification.\")\n",
    "\n",
    "    # nominal, lower and upper bounds for the forward pass\n",
    "    y_n = model.forward(batch).squeeze()\n",
    "\n",
    "    # transform 2-logit models to a single output\n",
    "    if y_n.shape[-1] == 2:\n",
    "        y_n = y_n[:, 1] - y_n[:, 0]\n",
    "    if y_n.dim() > 1:\n",
    "        raise NotImplementedError(\"Noisy accuracy is not supported for multi-class classification.\")\n",
    "\n",
    "    # # apply noise + threshold dp mechanisim\n",
    "    # y_n = (logit_n > 0).to(torch.float32).squeeze()\n",
    "    # noise = noise_distribution.sample().to(y_n.device).squeeze()\n",
    "    # assert noise.shape == y_n.shape\n",
    "    # y_n = (y_n + noise) > 0.5\n",
    "    # accuracy = (y_n == labels).float().mean().item()\n",
    "\n",
    "    # apply noise + threshold dp mechanisim\n",
    "    noise = noise_distribution.sample().to(y_n.device).squeeze()\n",
    "    assert noise.shape == y_n.shape\n",
    "    y_n = y_n + noise\n",
    "    accuracy = F.mse_loss(y_n, labels.squeeze()).item()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70cf0cbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory models/18epochs does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m k_private_values:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mprivacy_bounded_models\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_params\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/18epochs/uci_k\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mk\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/pt321/.venv/lib/python3.10/site-packages/abstract_gradient_training/bounded_models/base_model.py:254\u001b[0m, in \u001b[0;36mBoundedModel.save_params\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msave_params\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m    Save the model parameters to a pytorch file.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03m        filename (str): Path to the file where the parameters should be saved.\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparam_n\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_param_n\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparam_l\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_param_l\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparam_u\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_param_u\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/pt321/.venv/lib/python3.10/site-packages/torch/serialization.py:943\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    940\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 943\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    944\u001b[0m         _save(\n\u001b[1;32m    945\u001b[0m             obj,\n\u001b[1;32m    946\u001b[0m             opened_zipfile,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    949\u001b[0m             _disable_byteorder_record,\n\u001b[1;32m    950\u001b[0m         )\n\u001b[1;32m    951\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/vol/bitbucket/pt321/.venv/lib/python3.10/site-packages/torch/serialization.py:810\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    809\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 810\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/pt321/.venv/lib/python3.10/site-packages/torch/serialization.py:781\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    778\u001b[0m         torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream, _compute_crc32)\n\u001b[1;32m    779\u001b[0m     )\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 781\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_compute_crc32\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parent directory models/18epochs does not exist."
     ]
    }
   ],
   "source": [
    "for k in k_private_values:\n",
    "    privacy_bounded_models[k].save_params(f\"models/18epochs/uci_k{k}.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990b453a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2549, 0.2549, 0.2549,  ..., 0.2549, 0.2549, 0.2549], device='cuda:0')\n",
      "1.588772794591271\n",
      "Accuracy using AGT smooth sensitivity bounds: 325584.03\n",
      "Average MSE is 190.5872059983658\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import privacy_utils_regression\n",
    "importlib.reload(privacy_utils_regression)\n",
    "\n",
    "epsilon = 1.0\n",
    "# make privacy-safe predictions using the smooth sensitivity bounds from AGT\n",
    "noise_level = privacy_utils_regression.get_calibrated_noise_level(\n",
    "    test_data.tensors[0], privacy_bounded_models, min_bound=0, max_bound=10000, epsilon=epsilon, noise_type=\"cauchy\" \n",
    ")\n",
    "print(noise_level)\n",
    "accuracy = noisy_test_mse(\n",
    "    bounded_model, *test_data.tensors, noise_level=noise_level, noise_type=\"cauchy\"\n",
    ")\n",
    "print(accuracy / len(test_data))\n",
    "print(f\"Accuracy using AGT smooth sensitivity bounds: {accuracy:.2f}\")\n",
    "\n",
    "ave = 0\n",
    "num = 10000\n",
    "for i in range(num):\n",
    "    ave += noisy_test_mse(\n",
    "        bounded_model, *test_data.tensors, noise_level=noise_level, noise_type=\"cauchy\"\n",
    "    )\n",
    "print(f\"Average MSE is {ave / (num * len(test_data))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
